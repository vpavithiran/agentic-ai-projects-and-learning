{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8f7041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond as ChatGPT. The user says \"hi\". Should respond friendly. No policy issues.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 72, 'total_tokens': 113, 'completion_time': 0.081474879, 'prompt_time': 0.002441754, 'queue_time': 0.046530765, 'total_time': 0.083916633}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--dfe4396d-8686-4a4f-967f-4ccfaa7450ba-0', usage_metadata={'input_tokens': 72, 'output_tokens': 41, 'total_tokens': 113})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\", api_key=groq_api_key)\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b910064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal, TypedDict\n",
    "import operator\n",
    "from typing import Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4f3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    name:str=Field(description=\"Name for this section of the report\")\n",
    "    description:str=Field(description=\"Brief Overview of main topics and concepts of the section\")\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections:List[Section]=Field(description=\"Sections of the report\")\n",
    "\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49d2caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.types import Send\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic:str\n",
    "    sections:list[Sections]\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]\n",
    "    final_report: str\n",
    "\n",
    "class WorkerState(TypedDict):\n",
    "    section:Section\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bbf932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state:State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "    report_sections = planner.invoke(\n",
    "        \n",
    "           [ SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state[\"topic\"]}\"),]\n",
    "        \n",
    "    )\n",
    "    print(\"Report Section:\", report_sections)\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "def llm_call(state:WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "    section = llm.invoke(\n",
    "        \n",
    "            [SystemMessage(content=\"Write a report section following the provided name and description. Include no preamble for each section.use Markdown formatting.\"),\n",
    "            HumanMessage(content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"),\n",
    "        ]\n",
    "    )\n",
    "    return{\"completed_sections\": [section.content]}\n",
    "\n",
    "def assign_workers(state: State):\n",
    "    \"\"\" Assign a worker to each section in the plan\"\"\"\n",
    "    return [Send(\"llm_call\", {\"section\":s}) for s in state[\"sections\"]]\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\" Synthesize full report from sections\"\"\"\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return {\"final_report\": completed_report_sections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf37a81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import display, Image\n",
    "\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"llm_call\"])\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21deee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Section: sections=[Section(name='Executive Summary', description='A concise overview of the report, highlighting the key machine learning concepts, their significance, and the main takeaways for readers.'), Section(name='Introduction to Machine Learning', description='Defines machine learning, its historical evolution, relationship to AI and statistics, and outlines the scope of the report.'), Section(name='Fundamental Concepts', description='Covers core ideas such as data representation, features, labels, training vs. testing, overfitting/underfitting, bias‑variance trade‑off, and the learning pipeline.'), Section(name='Supervised Learning', description='Explains the paradigm, key algorithms (linear regression, logistic regression, decision trees, SVM, k‑NN, neural networks), loss functions, and typical applications.'), Section(name='Unsupervised Learning', description='Describes clustering, dimensionality reduction, density estimation, and algorithms like k‑means, hierarchical clustering, PCA, t‑SNE, and autoencoders.'), Section(name='Reinforcement Learning', description='Introduces agents, environments, reward signals, policies, value functions, and popular methods such as Q‑learning, SARSA, and deep RL (DQN, policy gradients).'), Section(name='Model Evaluation & Validation', description='Discusses metrics (accuracy, precision, recall, F1, ROC‑AUC, RMSE, etc.), cross‑validation, confusion matrix, and model selection techniques.'), Section(name='Practical Considerations & Best Practices', description='Covers data preprocessing, feature engineering, hyperparameter tuning, regularization, scalability, and reproducibility.'), Section(name='Current Trends & Emerging Topics', description='Highlights deep learning, transfer learning, federated learning, explainable AI, ethical AI, and automated machine learning (AutoML).'), Section(name='Challenges and Limitations', description='Analyzes issues such as data bias, interpretability, computational cost, privacy concerns, and model robustness.'), Section(name='Future Outlook', description='Speculates on upcoming research directions, industry adoption, and societal impact of machine learning technologies.'), Section(name='Conclusion', description='Summarizes the main points, reiterates the importance of mastering machine‑learning concepts, and provides recommendations for further study.'), Section(name='References', description='A curated list of books, research papers, online courses, and reputable resources for deeper exploration.')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Executive Summary\n",
       "\n",
       "This report provides a concise overview of foundational and emerging machine‑learning concepts, emphasizing their practical significance and the strategic insights they deliver to organizations and practitioners.\n",
       "\n",
       "- **Core Concepts**  \n",
       "  - **Supervised Learning** – Predictive modeling using labeled data (e.g., regression, classification).  \n",
       "  - **Unsupervised Learning** – Discovering hidden structure without explicit labels (e.g., clustering, dimensionality reduction).  \n",
       "  - **Reinforcement Learning** – Decision‑making through interaction with an environment and reward feedback.  \n",
       "  - **Deep Learning** – Hierarchical neural networks that excel in perception tasks such as image, speech, and language processing.  \n",
       "  - **Transfer & Few‑Shot Learning** – Leveraging pre‑trained models to adapt quickly to new tasks with minimal data.\n",
       "\n",
       "- **Significance**  \n",
       "  - Enables data‑driven automation, improving efficiency and reducing human error.  \n",
       "  - Drives competitive advantage by uncovering actionable insights from large, complex datasets.  \n",
       "  - Facilitates personalization at scale, enhancing customer experience across industries.  \n",
       "  - Supports real‑time decision making in critical domains (e.g., finance, healthcare, autonomous systems).\n",
       "\n",
       "- **Main Takeaways**  \n",
       "  1. **Strategic Alignment** – Successful ML initiatives require clear business objectives, robust data pipelines, and cross‑functional collaboration.  \n",
       "  2. **Model Selection & Evaluation** – Choose algorithms that match the problem’s complexity, data availability, and interpretability needs; rigorously validate with appropriate metrics and cross‑validation.  \n",
       "  3. **Ethics & Governance** – Proactively address bias, privacy, and regulatory compliance to build trustworthy AI systems.  \n",
       "  4. **Scalability & Maintenance** – Deploy models using containerization, CI/CD pipelines, and monitoring to ensure performance over time.  \n",
       "  5. **Continuous Learning** – Stay abreast of rapid advances (e.g., foundation models, prompt engineering) to sustain innovation.\n",
       "\n",
       "Overall, mastering these concepts equips readers to design, implement, and govern machine‑learning solutions that deliver measurable value while navigating technical and ethical challenges.\n",
       "\n",
       "---\n",
       "\n",
       "## Introduction to Machine Learning\n",
       "\n",
       "Machine learning (ML) is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models enabling computers to perform tasks without explicit programming. By learning patterns from data, ML systems can make predictions, classify information, and uncover insights that would be infeasible to encode manually.\n",
       "\n",
       "### Historical Evolution\n",
       "- **1950s–1960s:** Early concepts emerged with the perceptron and symbolic AI approaches, emphasizing rule‑based reasoning.\n",
       "- **1970s–1980s:** Introduction of decision trees, nearest‑neighbor methods, and the back‑propagation algorithm for neural networks revived interest in data‑driven learning.\n",
       "- **1990s:** The rise of support vector machines, ensemble methods (e.g., boosting, bagging), and the proliferation of statistical learning theory formalized the field.\n",
       "- **2000s:** Availability of large datasets and increased computational power spurred the resurgence of deep neural networks, leading to breakthroughs in computer vision, speech, and natural language processing.\n",
       "- **2010s–present:** Integration of reinforcement learning, transfer learning, and generative models has expanded ML applications across industry, science, and everyday technology.\n",
       "\n",
       "### Relationship to AI and Statistics\n",
       "- **Artificial Intelligence:** ML is a core engine of modern AI, providing the learning capability that differentiates adaptive systems from static, rule‑based AI.\n",
       "- **Statistics:** ML shares many foundations with statistical inference—probability theory, hypothesis testing, and model evaluation—but places a stronger emphasis on prediction accuracy, scalability, and algorithmic efficiency.\n",
       "\n",
       "### Scope of This Report\n",
       "This report surveys the principal paradigms and techniques of machine learning, including:\n",
       "1. **Supervised Learning:** Regression, classification, and ensemble methods.\n",
       "2. **Unsupervised Learning:** Clustering, dimensionality reduction, and density estimation.\n",
       "3. **Reinforcement Learning:** Policy optimization and value‑based methods.\n",
       "4. **Deep Learning:** Convolutional, recurrent, and transformer architectures.\n",
       "5. **Practical Considerations:** Data preprocessing, model evaluation, interpretability, and ethical implications.\n",
       "\n",
       "By exploring these areas, the report aims to equip readers with a comprehensive understanding of how machine learning operates, its historical context, and its pivotal role within the broader AI ecosystem.\n",
       "\n",
       "---\n",
       "\n",
       "## Fundamental Concepts\n",
       "\n",
       "### Data Representation\n",
       "- **Raw Data**: The original observations collected from sensors, databases, or user inputs.\n",
       "- **Structured Format**: Typically organized into a matrix \\(X \\in \\mathbb{R}^{n \\times p}\\) where \\(n\\) is the number of samples and \\(p\\) the number of features.\n",
       "- **Numerical Encoding**: Categorical variables are transformed (e.g., one‑hot, ordinal encoding) to enable mathematical operations.\n",
       "\n",
       "### Features and Labels\n",
       "- **Features (Inputs)**: The columns of \\(X\\); each captures a measurable attribute that may influence the target.\n",
       "- **Labels (Outputs/Targets)**: The vector \\(y \\in \\mathbb{R}^{n}\\) (regression) or \\(y \\in \\{0,1\\}^{n}\\) (classification) that the model aims to predict.\n",
       "- **Feature Engineering**: Process of creating, selecting, or transforming features to improve model expressiveness (e.g., scaling, interaction terms, dimensionality reduction).\n",
       "\n",
       "### Training vs. Testing\n",
       "| Phase      | Purpose                                    | Data Used                         |\n",
       "|------------|--------------------------------------------|-----------------------------------|\n",
       "| **Training** | Fit model parameters \\(\\theta\\) by minimizing a loss on known pairs \\((X_{\\text{train}}, y_{\\text{train}})\\). | Subset of data (typically 70‑80 % of total). |\n",
       "| **Testing**  | Evaluate generalization performance on unseen data. | Held‑out subset (typically 20‑30 %). |\n",
       "- **Validation Set** (optional): Used for hyper‑parameter tuning without contaminating the test set.\n",
       "\n",
       "### Overfitting & Underfitting\n",
       "- **Underfitting**: Model is too simple; high bias; poor performance on both training and test data.\n",
       "- **Overfitting**: Model captures noise; high variance; excellent training performance but degraded test performance.\n",
       "- **Symptoms**: Large gap between training and validation error indicates overfitting; both errors high indicate underfitting.\n",
       "\n",
       "### Bias‑Variance Trade‑off\n",
       "- **Bias**: Error introduced by approximating a real‑world problem with a simplified model.\n",
       "- **Variance**: Error due to sensitivity to fluctuations in the training set.\n",
       "- **Decomposition**: Expected squared error = \\(\\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}\\).\n",
       "- **Goal**: Choose model complexity that balances bias and variance to minimize total error.\n",
       "\n",
       "### Learning Pipeline\n",
       "1. **Data Collection** – Gather raw observations from relevant sources.\n",
       "2. **Pre‑processing** – Clean, handle missing values, encode categorical variables, normalize/scale.\n",
       "3. **Feature Engineering** – Create informative features, reduce dimensionality (PCA, feature selection).\n",
       "4. **Dataset Split** – Partition into training, validation, and test sets (or use cross‑validation).\n",
       "5. **Model Selection** – Choose algorithmic family (linear, tree‑based, neural network, etc.).\n",
       "6. **Training** – Optimize model parameters on the training set.\n",
       "7. **Hyper‑parameter Tuning** – Adjust settings (regularization strength, depth, learning rate) using validation data or automated search (grid/random/Bayesian).\n",
       "8. **Evaluation** – Compute performance metrics (accuracy, RMSE, AUC, etc.) on the test set.\n",
       "9. **Deployment** – Export the trained model, monitor real‑world performance, and trigger retraining when drift is detected.\n",
       "10. **Maintenance** – Periodic re‑evaluation, data updates, and model versioning.\n",
       "\n",
       "Understanding these foundational concepts equips practitioners to design robust machine‑learning solutions that generalize well beyond the data they were trained on.\n",
       "\n",
       "---\n",
       "\n",
       "## Supervised Learning\n",
       "\n",
       "Supervised learning is a machine‑learning paradigm in which a model is trained on a labeled dataset — each training example consists of an input vector **x** and a corresponding target label **y** (either a scalar, a vector, or a class). The goal is to learn a mapping *f(x; θ)* parameterized by θ that can predict **y** for previously unseen inputs, minimizing a predefined loss function that quantifies prediction error.\n",
       "\n",
       "### Core Concepts\n",
       "| Element | Description |\n",
       "|---------|-------------|\n",
       "| **Training data** | Set 𝔻 = {(**x**ᵢ, **y**ᵢ)}ᵢ₌₁ᴺ, where **y**ᵢ are known ground‑truth values. |\n",
       "| **Hypothesis space** | Family of functions (e.g., linear models, trees, neural nets) from which the learner selects the best candidate. |\n",
       "| **Loss function** | Measures discrepancy between predicted and true labels; the choice of loss drives the learning algorithm. |\n",
       "| **Optimization** | Adjusts parameters θ to minimize the empirical risk 𝑅̂(θ) = (1/N)∑ₙ ℓ(f(**x**ₙ;θ), **y**ₙ). |\n",
       "| **Generalization** | Ability of the learned model to perform well on new, unseen data, assessed via validation/test sets or cross‑validation. |\n",
       "\n",
       "### Key Algorithms & Typical Losses\n",
       "\n",
       "| Algorithm | Typical Use‑Case | Model Form | Common Loss Functions |\n",
       "|-----------|------------------|------------|-----------------------|\n",
       "| **Linear Regression** | Predicting continuous outcomes (e.g., house prices) | **f(x)=wᵀx + b** (affine) | Mean Squared Error (MSE): ℓ = (y‑ŷ)² |\n",
       "| **Logistic Regression** | Binary or multinomial classification (e.g., spam detection) | **f(x)=σ(wᵀx + b)**, σ = sigmoid | Cross‑entropy (log‑loss): ℓ = −[y·log p + (1‑y)·log (1‑p)] |\n",
       "| **Decision Trees** | Interpretable classification/regression (e.g., credit scoring) | Hierarchical partition of feature space | Gini impurity / Entropy for classification; MSE for regression |\n",
       "| **Support Vector Machines (SVM)** | High‑dimensional classification, margin‑maximizing tasks | Linear or kernel‑induced hyperplane | Hinge loss: ℓ = max(0, 1‑y·f(x)) |\n",
       "| **k‑Nearest Neighbors (k‑NN)** | Instance‑based learning where locality matters (e.g., recommendation) | Non‑parametric, prediction by majority vote (classification) or average (regression) | No explicit loss; distance metric (e.g., Euclidean) drives prediction |\n",
       "| **Neural Networks (MLP, CNN, RNN, etc.)** | Complex patterns such as image, speech, and sequential data | Stacked layers of nonlinear transformations | MSE for regression, cross‑entropy for classification, custom losses (e.g., CTC for speech) |\n",
       "\n",
       "#### Loss Function Highlights\n",
       "- **Mean Squared Error (MSE)** – penalizes large deviations, smooth gradient, suited for regression.\n",
       "- **Mean Absolute Error (MAE)** – robust to outliers, gradient is constant sign.\n",
       "- **Cross‑Entropy** – measures divergence between true label distribution and predicted probabilities; encourages calibrated probabilities.\n",
       "- **Hinge Loss** – focuses on margin violations; central to SVMs.\n",
       "- **Huber Loss** – combines MSE (near zero error) and MAE (far from zero) for robustness.\n",
       "\n",
       "### Typical Applications\n",
       "- **Finance:** Credit risk scoring (logistic regression, decision trees), algorithmic trading signals (gradient‑boosted trees, neural nets).\n",
       "- **Healthcare:** Disease diagnosis from imaging (CNNs), patient outcome prediction (random forests, survival models).\n",
       "- **Natural Language Processing:** Sentiment analysis (logistic regression, LSTM/Transformer models), spam filtering (SVM, Naïve Bayes).\n",
       "- **Computer Vision:** Object detection and classification (CNNs, ResNet, YOLO), image regression (depth estimation with fully‑connected nets).\n",
       "- **Recommender Systems:** Rating prediction (matrix factorization with supervised fine‑tuning), click‑through‑rate prediction (gradient‑boosted decision trees).\n",
       "- **Manufacturing & IoT:** Predictive maintenance (random forests, gradient boosting), sensor fault detection (k‑NN, SVM).\n",
       "\n",
       "### Practical Workflow\n",
       "1. **Data collection & labeling** – ensure high‑quality, representative labels.\n",
       "2. **Pre‑processing** – handling missing values, feature scaling (standardization or min‑max), categorical encoding.\n",
       "3. **Model selection** – choose algorithm(s) based on problem type, data size, interpretability needs.\n",
       "4. **Training** – minimize the chosen loss via gradient‑based optimizers (SGD, Adam) or closed‑form solutions (ordinary least squares for linear regression).\n",
       "5. **Validation** – use cross‑validation or a hold‑out set to tune hyper‑parameters (e.g., regularization λ, tree depth, k in k‑NN).\n",
       "6. **Evaluation** – appropriate metrics: RMSE / MAE for regression, accuracy / precision‑recall / ROC‑AUC for classification.\n",
       "7. **Deployment & monitoring** – export model (ONNX, TensorFlow SavedModel), monitor drift, and schedule periodic retraining.\n",
       "\n",
       "Supervised learning remains the workhorse of modern AI, providing a clear, data‑driven pathway from raw observations to actionable predictions across virtually every industry.\n",
       "\n",
       "---\n",
       "\n",
       "## Unsupervised Learning\n",
       "\n",
       "Unsupervised learning tackles tasks where the data lack explicit target labels. The goal is to uncover hidden structure, relationships, or distributions within the data. Core problem families include **clustering**, **dimensionality reduction**, and **density estimation**, each supported by a suite of algorithms.\n",
       "\n",
       "### 1. Clustering  \n",
       "Clustering partitions a dataset into groups (clusters) such that intra‑cluster similarity is high and inter‑cluster similarity is low.\n",
       "\n",
       "| Method | Principle | Typical Use‑Cases |\n",
       "|--------|-----------|-------------------|\n",
       "| **k‑means** | Minimizes within‑cluster sum of squares (WCSS) by iteratively assigning points to the nearest centroid and updating centroids. | Large‑scale segmentation, image quantization, market segmentation. |\n",
       "| **Hierarchical clustering** | Builds a dendrogram by either agglomerative (bottom‑up) merging or divisive (top‑down) splitting based on a linkage criterion (single, complete, average, Ward). | Phylogenetic analysis, document taxonomy, exploratory data analysis where cluster hierarchy is valuable. |\n",
       "| **DBSCAN** (Density‑Based Spatial Clustering of Applications with Noise) | Groups points reachable within a radius ε with at least *minPts* neighbors; isolates low‑density points as noise. | Spatial data, anomaly detection, data with irregular shapes. |\n",
       "| **Spectral clustering** | Constructs a similarity graph, computes the Laplacian eigenvectors, and runs k‑means in the reduced spectral space. | Community detection in networks, image segmentation. |\n",
       "\n",
       "#### Practical considerations\n",
       "- **Choosing *k***: Elbow method, silhouette score, or information‑theoretic criteria (e.g., BIC) help select an appropriate number of clusters.\n",
       "- **Scaling**: Standardize or normalize features before distance‑based clustering to avoid dominance by high‑variance dimensions.\n",
       "- **Initialization**: k‑means++ seeding reduces sensitivity to random starts.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Dimensionality Reduction  \n",
       "Dimensionality reduction transforms high‑dimensional data into a lower‑dimensional representation while preserving salient structure.\n",
       "\n",
       "| Technique | Objective | Key Characteristics |\n",
       "|-----------|-----------|----------------------|\n",
       "| **Principal Component Analysis (PCA)** | Linear projection onto orthogonal axes that maximize variance. Solved via eigen‑decomposition of the covariance matrix or singular value decomposition (SVD). | Fast, globally optimal for linear variance preservation, interpretable components. |\n",
       "| **t‑Distributed Stochastic Neighbor Embedding (t‑SNE)** | Non‑linear embedding that preserves local pairwise similarities by minimizing Kullback‑Leibler divergence between high‑dimensional and low‑dimensional similarity distributions. | Excellent for visualizing clusters; computationally intensive; perplexity hyperparameter controls neighborhood size. |\n",
       "| **Uniform Manifold Approximation and Projection (UMAP)** | Manifold learning based on fuzzy simplicial set construction; optimizes cross‑entropy between high‑ and low‑dimensional fuzzy topological representations. | Faster than t‑SNE, preserves both local and some global structure, scalable to millions of points. |\n",
       "| **Autoencoders** | Neural networks trained to reconstruct inputs; the bottleneck layer yields a compressed representation. Variants include variational autoencoders (VAEs) for probabilistic embeddings. | Captures non‑linear structure, can be tailored with convolutional or recurrent layers for specific data modalities. |\n",
       "\n",
       "#### Example: PCA derivation (brief)\n",
       "Given centered data matrix **X** ∈ ℝ^{n×d}, the covariance matrix is **C** = (1/(n‑1)) **X**ᵀ**X**. Eigen‑decomposition **C** = **UΛU**ᵀ yields eigenvectors **U** (principal axes) and eigenvalues **Λ** (explained variance). Projecting onto the first *p* eigenvectors gives the reduced data **Z** = **XU**_{:,1:p}.\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Density Estimation  \n",
       "Density estimation models the underlying probability distribution *p(x)* of the data without supervision.\n",
       "\n",
       "| Approach | Description | When to Use |\n",
       "|----------|-------------|-------------|\n",
       "| **Parametric (e.g., Gaussian Mixture Models – GMM)** | Assumes data arise from a mixture of *K* Gaussian components; parameters estimated via Expectation‑Maximization (EM). | When the data roughly follow multimodal Gaussian shapes and a closed‑form likelihood is needed. |\n",
       "| **Non‑parametric (e.g., Kernel Density Estimation – KDE)** | Places a kernel (commonly Gaussian) on each data point and sums them to approximate *p(x)*. Bandwidth selection crucial. | Small‑to‑moderate dimensional data where flexibility outweighs computational cost. |\n",
       "| **Neural density models (e.g., Normalizing Flows, VAEs)** | Learn invertible transformations or latent variable models to represent complex densities. | High‑dimensional data (images, audio) where expressive, tractable likelihoods are required. |\n",
       "\n",
       "#### GMM EM steps (summary)\n",
       "1. **E‑step**: Compute responsibilities γ_{ik} = \\frac{π_k 𝒩(x_i | μ_k, Σ_k)}{\\sum_{j=1}^{K} π_j 𝒩(x_i | μ_j, Σ_j)}.  \n",
       "2. **M‑step**: Update parameters  \n",
       "   - π_k = (1/n) ∑_i γ_{ik}  \n",
       "   - μ_k = \\frac{\\sum_i γ_{ik} x_i}{\\sum_i γ_{ik}}  \n",
       "   - Σ_k = \\frac{\\sum_i γ_{ik} (x_i‑μ_k)(x_i‑μ_k)ᵀ}{\\sum_i γ_{ik}}.\n",
       "\n",
       "---\n",
       "\n",
       "### 4. Representative Algorithms in Practice\n",
       "\n",
       "#### k‑means (pseudo‑code)\n",
       "```python\n",
       "def kmeans(X, k, max_iter=300):\n",
       "    centroids = init_centroids(X, k)          # e.g., k‑means++\n",
       "    for _ in range(max_iter):\n",
       "        # Assignment step\n",
       "        labels = np.argmin(distances(X, centroids), axis=1)\n",
       "        # Update step\n",
       "        new_centroids = np.array([X[labels==j].mean(axis=0) for j in range(k)])\n",
       "        if np.allclose(centroids, new_centroids):\n",
       "            break\n",
       "        centroids = new_centroids\n",
       "    return labels, centroids\n",
       "```\n",
       "\n",
       "#### Hierarchical agglomerative clustering (linkage)\n",
       "```python\n",
       "from scipy.cluster.hierarchy import linkage, fcluster\n",
       "Z = linkage(X, method='ward')          # builds dendrogram\n",
       "clusters = fcluster(Z, t=5, criterion='maxclust')\n",
       "```\n",
       "\n",
       "#### Autoencoder (Keras sketch)\n",
       "```python\n",
       "input_dim = X.shape[1]\n",
       "latent_dim = 32\n",
       "\n",
       "input_layer = Input(shape=(input_dim,))\n",
       "encoded = Dense(128, activation='relu')(input_layer)\n",
       "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
       "\n",
       "decoded = Dense(128, activation='relu')(encoded)\n",
       "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
       "\n",
       "autoencoder = Model(input_layer, decoded)\n",
       "autoencoder.compile(optimizer='adam', loss='mse')\n",
       "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### 5. Choosing the Right Unsupervised Technique\n",
       "\n",
       "| Data characteristic | Recommended approach |\n",
       "|---------------------|----------------------|\n",
       "| **Clear, spherical clusters, low‑dimensional** | k‑means (with careful *k* selection) |\n",
       "| **Irregularly shaped clusters or noise** | DBSCAN or hierarchical clustering |\n",
       "| **Need for a hierarchy of groups** | Agglomerative clustering with dendrogram analysis |\n",
       "| **Very high dimensional, linear structure** | PCA (or randomized SVD for scalability) |\n",
       "| **Visualization of complex manifolds** | t‑SNE or UMAP (tune perplexity / n_neighbors) |\n",
       "| **Non‑linear feature extraction** | Autoencoders / Variational Autoencoders |\n",
       "| **Probabilistic modeling of multimodal data** | Gaussian Mixture Models or Normalizing Flows |\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Common Pitfalls & Best Practices\n",
       "\n",
       "1. **Feature scaling** – Distance‑based methods (k‑means, hierarchical) are highly sensitive to scale; always standardize or normalize.\n",
       "2. **Curse of dimensionality** – In high dimensions, distances become less discriminative; apply dimensionality reduction before clustering.\n",
       "3. **Hyperparameter sensitivity** – Algorithms like DBSCAN (ε, minPts) and t‑SNE (perplexity, learning rate) require systematic tuning, often via grid search or visual inspection.\n",
       "4. **Interpretability vs. performance** – Linear methods (PCA) are easier to interpret, while deep autoencoders may yield better compression at the cost of opacity.\n",
       "5. **Evaluation without labels** – Use internal metrics (silhouette score, Davies‑Bouldin index) and stability analysis (e.g., bootstrapping) to assess clustering quality.\n",
       "\n",
       "---\n",
       "\n",
       "By mastering these unsupervised techniques, practitioners can extract meaningful patterns, reduce computational burden, and lay the groundwork for downstream supervised tasks or exploratory insights.\n",
       "\n",
       "---\n",
       "\n",
       "## Reinforcement Learning\n",
       "\n",
       "Reinforcement Learning (RL) studies how an **agent** interacts with an **environment** to achieve a goal defined by a **reward signal**. At each time step *t*, the agent observes a state *sₜ*, selects an action *aₜ* according to its **policy** π, receives a scalar reward *rₜ₊₁*, and transitions to a new state *sₜ₊₁*. The objective is to learn a policy that maximizes the expected cumulative discounted return  \n",
       "\n",
       "\\[\n",
       "G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1},\n",
       "\\]\n",
       "\n",
       "where γ ∈ [0,1] is the discount factor.\n",
       "\n",
       "### Core Concepts  \n",
       "\n",
       "| Concept | Definition |\n",
       "|---------|------------|\n",
       "| **Agent** | The decision‑making entity that selects actions. |\n",
       "| **Environment** | The external system with which the agent interacts; it supplies states and rewards. |\n",
       "| **Reward Signal** | Immediate scalar feedback indicating the desirability of a transition. |\n",
       "| **Policy (π)** | Mapping from states to a probability distribution over actions (π(a|s)). Can be deterministic (a = μ(s)) or stochastic. |\n",
       "| **Value Functions** | Quantify expected return: <br>• **State‑value** Vπ(s) = 𝔼π[Gₜ | sₜ = s] <br>• **Action‑value** Qπ(s,a) = 𝔼π[Gₜ | sₜ = s, aₜ = a]. |\n",
       "| **Model** (optional) | A representation of the environment dynamics *p(s' , r | s, a)* used for planning. |\n",
       "\n",
       "### Classical Tabular Methods  \n",
       "\n",
       "1. **Q‑Learning** (off‑policy)  \n",
       "   \\[\n",
       "   Q(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\big]\n",
       "   \\]  \n",
       "   - Learns the optimal action‑value function *Q\\** independent of the behavior policy.  \n",
       "   - Converges to *Q\\** under appropriate learning‑rate schedules and sufficient exploration.\n",
       "\n",
       "2. **SARSA** (on‑policy)  \n",
       "   \\[\n",
       "   Q(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma Q(s',a') - Q(s,a) \\big]\n",
       "   \\]  \n",
       "   - Updates using the action *a'* actually taken by the current policy, preserving the on‑policy nature.  \n",
       "\n",
       "Both algorithms can be combined with **ε‑greedy** exploration, where the agent selects a random action with probability ε and the greedy action otherwise.\n",
       "\n",
       "### Deep Reinforcement Learning  \n",
       "\n",
       "When the state or action space is large or continuous, tabular representations become infeasible. **Deep RL** employs neural networks as function approximators for value functions or policies.\n",
       "\n",
       "#### Deep Q‑Network (DQN)  \n",
       "\n",
       "- Approximates the action‑value function with a deep network *Q(s,a; θ)*.  \n",
       "- Key techniques:  \n",
       "  - **Experience Replay**: Stores transitions in a buffer and samples mini‑batches to break correlation.  \n",
       "  - **Target Network**: A separate network *θ⁻* provides stable target values, updated periodically.  \n",
       "- Update rule (mini‑batch loss):  \n",
       "  \\[\n",
       "  L(\\theta) = \\mathbb{E}_{(s,a,r,s')}\\Big[ \\big(r + \\gamma \\max_{a'} Q(s',a';\\theta^{-}) - Q(s,a;\\theta) \\big)^2 \\Big].\n",
       "  \\]\n",
       "\n",
       "#### Policy Gradient Methods  \n",
       "\n",
       "Instead of estimating value functions, these methods directly optimize the policy parameters θ by gradient ascent on the expected return:\n",
       "\n",
       "\\[\n",
       "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\big[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, G_t \\big].\n",
       "\\]\n",
       "\n",
       "Common variants:\n",
       "\n",
       "- **REINFORCE** (Monte‑Carlo policy gradient) – uses full episode returns as the advantage estimator.  \n",
       "- **Actor‑Critic** – combines a policy (actor) with a learned value function (critic) to reduce variance:  \n",
       "  \\[\n",
       "  \\nabla_\\theta J \\approx \\mathbb{E}\\big[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, (r + \\gamma V_w(s') - V_w(s)) \\big].\n",
       "  \\]  \n",
       "\n",
       "#### Advanced Deep RL Algorithms  \n",
       "\n",
       "| Algorithm | Core Idea |\n",
       "|-----------|-----------|\n",
       "| **DDPG** (Deep Deterministic Policy Gradient) | Extends policy gradients to continuous actions using an actor‑critic with deterministic policies. |\n",
       "| **A3C / A2C** (Asynchronous Advantage Actor‑Critic) | Parallel workers collect experience; the advantage function A(s,a) = Q(s,a) – V(s) stabilizes learning. |\n",
       "| **PPO** (Proximal Policy Optimization) | Clips policy updates to stay within a trust region, offering a simple yet robust alternative to TRPO. |\n",
       "| **SAC** (Soft Actor‑Critic) | Maximizes a trade‑off between expected return and entropy, encouraging exploration. |\n",
       "\n",
       "### Summary  \n",
       "\n",
       "Reinforcement Learning provides a unified framework for sequential decision making. Starting from simple tabular algorithms such as Q‑learning and SARSA, the field has progressed to deep architectures that can handle high‑dimensional perceptual inputs and continuous control tasks. The central pillars—agents, environments, reward signals, policies, and value functions—remain the same, while modern methods enrich them with function approximation, stability tricks, and sophisticated policy‑optimization techniques.\n",
       "\n",
       "---\n",
       "\n",
       "## Model Evaluation & Validation\n",
       "\n",
       "### Key Performance Metrics  \n",
       "\n",
       "| Metric | Typical Use‑Case | Interpretation |\n",
       "|--------|------------------|----------------|\n",
       "| **Accuracy** | Balanced classification problems | Proportion of correctly classified instances. |\n",
       "| **Precision** | Scenarios where false positives are costly (e.g., spam detection) | Ratio of true positives to all predicted positives. |\n",
       "| **Recall (Sensitivity)** | Situations where missing a positive is critical (e.g., disease screening) | Ratio of true positives to all actual positives. |\n",
       "| **F1‑Score** | Imbalanced classification where both precision and recall matter | Harmonic mean of precision and recall; balances the two. |\n",
       "| **ROC‑AUC** | Binary classifiers with varying decision thresholds | Area under the Receiver Operating Characteristic curve; measures discriminative ability across thresholds. |\n",
       "| **RMSE (Root Mean Squared Error)** | Regression tasks | Square root of average squared prediction error; penalizes large deviations. |\n",
       "| **MAE (Mean Absolute Error)** | Regression tasks where outliers are less critical | Average absolute difference between predictions and true values. |\n",
       "| **R² (Coefficient of Determination)** | Regression model goodness‑of‑fit | Proportion of variance in the target explained by the model. |\n",
       "\n",
       "### Confusion Matrix  \n",
       "\n",
       "A confusion matrix provides a granular view of classification outcomes:\n",
       "\n",
       "|                | **Predicted Positive** | **Predicted Negative** |\n",
       "|----------------|------------------------|------------------------|\n",
       "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
       "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
       "\n",
       "From the matrix we derive precision, recall, specificity, and other derived metrics. Visualizing the matrix (heatmap) helps spot systematic misclassifications.\n",
       "\n",
       "### Cross‑Validation Strategies  \n",
       "\n",
       "| Strategy | Description | When to Use |\n",
       "|----------|-------------|-------------|\n",
       "| **k‑Fold CV** | Data split into *k* equally sized folds; each fold serves as validation once. | General purpose; balances bias‑variance trade‑off. |\n",
       "| **Stratified k‑Fold** | Maintains class distribution within each fold. | Classification with imbalanced classes. |\n",
       "| **Leave‑One‑Out (LOO)** | Each observation is a validation set. | Very small datasets; high variance acceptable. |\n",
       "| **Time‑Series CV (Rolling/Expanding Window)** | Preserves temporal order; training set grows forward in time. | Forecasting or any sequential data. |\n",
       "| **Nested CV** | Outer loop for model assessment, inner loop for hyper‑parameter tuning. | Prevents optimistic bias when tuning many hyper‑parameters. |\n",
       "\n",
       "**Practical tip:** Report the mean and standard deviation of the chosen metric across folds to convey stability.\n",
       "\n",
       "### Model Selection Techniques  \n",
       "\n",
       "1. **Grid Search** – Exhaustive exploration of a predefined hyper‑parameter grid.  \n",
       "2. **Random Search** – Random sampling of hyper‑parameter space; often more efficient for high‑dimensional spaces.  \n",
       "3. **Bayesian Optimization** – Probabilistic model (e.g., Gaussian Process) guides the search toward promising regions.  \n",
       "4. **Successive Halving / Hyperband** – Allocates resources adaptively, discarding poor configurations early.  \n",
       "5. **Ensemble Methods** – Combine multiple models (bagging, boosting, stacking) to improve predictive performance and robustness.  \n",
       "\n",
       "**Selection criteria:**  \n",
       "- Primary metric aligned with business objective (e.g., maximize recall for safety‑critical alerts).  \n",
       "- Model complexity vs. interpretability trade‑off.  \n",
       "- Computational cost and latency constraints for deployment.  \n",
       "\n",
       "### Validation Workflow Summary  \n",
       "\n",
       "1. **Define target metric(s)** based on problem requirements.  \n",
       "2. **Partition data** using an appropriate CV scheme (stratified k‑fold for classification, time‑series CV for temporal data).  \n",
       "3. **Train candidate models** with hyper‑parameter tuning (grid/random/Bayesian).  \n",
       "4. **Evaluate** each candidate on validation folds; capture mean ± std of metrics, confusion matrices, and ROC curves.  \n",
       "5. **Select** the model that best balances performance, stability, and operational constraints.  \n",
       "6. **Perform final assessment** on an untouched test set (or via nested CV) to obtain an unbiased estimate of real‑world performance.  \n",
       "\n",
       "By systematically applying these metrics, validation strategies, and selection techniques, we ensure that the chosen model is both statistically sound and aligned with the project’s practical goals.\n",
       "\n",
       "---\n",
       "\n",
       "## Practical Considerations & Best Practices\n",
       "\n",
       "### 1. Data Preprocessing\n",
       "- **Data Cleaning**  \n",
       "  - Remove or impute missing values (e.g., median imputation for numeric features, most‑frequent for categorical).  \n",
       "  - Detect and handle outliers using robust statistics (IQR, MAD) or domain‑specific thresholds.  \n",
       "- **Normalization & Scaling**  \n",
       "  - Apply standard scaling (zero mean, unit variance) for algorithms sensitive to feature magnitude (e.g., SVM, neural nets).  \n",
       "  - Use min‑max scaling for bounded inputs or when preserving zero values is important.  \n",
       "- **Encoding Categorical Variables**  \n",
       "  - One‑hot encode low‑cardinality variables; use target encoding or embeddings for high‑cardinality features.  \n",
       "- **Temporal & Leakage Checks**  \n",
       "  - Ensure training data precedes validation/test data in time‑series contexts.  \n",
       "  - Exclude features that leak future information (e.g., post‑event indicators).  \n",
       "\n",
       "### 2. Feature Engineering\n",
       "- **Domain‑Driven Features**  \n",
       "  - Leverage domain knowledge to create interaction terms, ratios, or domain‑specific aggregates.  \n",
       "- **Automated Feature Construction**  \n",
       "  - Use tools like FeatureTools or polynomial feature generators for systematic expansion.  \n",
       "- **Dimensionality Reduction**  \n",
       "  - Apply PCA, t‑SNE, or autoencoders when the feature space is extremely high‑dimensional.  \n",
       "- **Feature Selection**  \n",
       "  - Filter methods (mutual information, chi‑square), wrapper methods (recursive feature elimination), or model‑based importance (tree SHAP).  \n",
       "\n",
       "### 3. Hyperparameter Tuning\n",
       "- **Search Strategies**  \n",
       "  - **Grid Search** for small, well‑understood parameter spaces.  \n",
       "  - **Random Search** for larger spaces; often yields near‑optimal results faster.  \n",
       "  - **Bayesian Optimization** (e.g., Optuna, Hyperopt) for efficient exploration of complex spaces.  \n",
       "- **Cross‑Validation Scheme**  \n",
       "  - Use stratified k‑fold for classification, time‑series split for sequential data.  \n",
       "  - Keep validation folds consistent across tuning runs to reduce variance.  \n",
       "- **Early Stopping & Warm‑Starts**  \n",
       "  - Integrate early stopping criteria for iterative models (GBMs, neural nets) to avoid over‑training.  \n",
       "  - Warm‑start models when expanding hyperparameter grids to reuse previous fits.  \n",
       "\n",
       "### 4. Regularization\n",
       "- **L1 & L2 Penalties**  \n",
       "  - L1 (Lasso) for sparsity; L2 (Ridge) for coefficient shrinkage.  \n",
       "  - Elastic Net combines both, balancing sparsity and stability.  \n",
       "- **Dropout & Batch Normalization** (Neural Networks)  \n",
       "  - Dropout mitigates co‑adaptation of neurons; batch norm stabilizes training dynamics.  \n",
       "- **Tree‑Based Model Controls**  \n",
       "  - Limit tree depth, set minimum child weight, use subsampling of rows/columns to reduce over‑fitting.  \n",
       "\n",
       "### 5. Scalability\n",
       "- **Data Partitioning**  \n",
       "  - Store data in columnar formats (Parquet, ORC) and leverage lazy loading (Dask, Spark).  \n",
       "- **Distributed Training**  \n",
       "  - Use frameworks like Spark MLlib, Horovod, or Ray Tune for parallel hyperparameter search.  \n",
       "- **Model Compression**  \n",
       "  - Apply quantization, pruning, or knowledge distillation for inference on resource‑constrained environments.  \n",
       "\n",
       "### 6. Reproducibility\n",
       "- **Version Control**  \n",
       "  - Track code, configuration files, and model artifacts with Git and DVC or MLflow.  \n",
       "- **Random Seed Management**  \n",
       "  - Set seeds for all libraries (NumPy, TensorFlow/PyTorch, scikit‑learn) and document them.  \n",
       "- **Environment Specification**  \n",
       "  - Capture dependencies via `requirements.txt`, `environment.yml`, or Docker containers.  \n",
       "- **Experiment Logging**  \n",
       "  - Log metrics, hyperparameters, and data versions to a central store (e.g., MLflow, Weights & Biases).  \n",
       "- **Automated Pipelines**  \n",
       "  - Encode the entire workflow (preprocessing → training → evaluation) in reproducible pipelines (Airflow, Prefect, Kubeflow).  \n",
       "\n",
       "By systematically addressing these practical aspects, teams can build models that are not only performant but also robust, scalable, and maintainable over the long term.\n",
       "\n",
       "---\n",
       "\n",
       "## Current Trends & Emerging Topics\n",
       "\n",
       "### Deep Learning\n",
       "- Continues to dominate AI research and applications, driven by larger model architectures (e.g., transformers) and ever‑increasing compute resources.  \n",
       "- Advances in model scaling (GPT‑4, PaLM, LLaMA) demonstrate emergent capabilities such as few‑shot learning and reasoning.  \n",
       "- Specialized hardware (TPUs, GPUs, AI accelerators) and software stacks (CUDA, ROCm, JAX) accelerate training and inference pipelines.\n",
       "\n",
       "### Transfer Learning\n",
       "- Pre‑trained models are fine‑tuned on domain‑specific data, reducing the need for massive labeled datasets.  \n",
       "- Techniques such as prompt‑tuning, adapters, and LoRA (Low‑Rank Adaptation) enable rapid customization while preserving core knowledge.  \n",
       "- Transfer learning is expanding beyond NLP to vision (ViT‑based models), speech, and multimodal domains.\n",
       "\n",
       "### Federated Learning\n",
       "- Enables collaborative model training across decentralized devices while keeping raw data local, addressing privacy and regulatory constraints.  \n",
       "- Recent work focuses on communication efficiency (compression, sparsification), robustness to non‑IID data, and secure aggregation protocols.  \n",
       "- Real‑world deployments are emerging in mobile keyboards, healthcare networks, and IoT edge ecosystems.\n",
       "\n",
       "### Explainable AI (XAI)\n",
       "- Provides transparency into black‑box models through post‑hoc methods (SHAP, LIME) and intrinsically interpretable architectures (attention maps, prototype networks).  \n",
       "- Regulatory pressure (e.g., EU AI Act) and industry demand for auditability are pushing XAI from research labs into production systems.  \n",
       "- Emerging standards aim to quantify explanation fidelity, user trust, and actionable insights.\n",
       "\n",
       "### Ethical AI\n",
       "- Addresses bias mitigation, fairness, accountability, and societal impact throughout the AI lifecycle.  \n",
       "- Frameworks such as AI Ethics Guidelines, Model Cards, and Datasheets for Datasets promote responsible development and documentation.  \n",
       "- Ongoing research explores algorithmic fairness metrics, causal analysis for bias detection, and governance mechanisms for AI oversight.\n",
       "\n",
       "### Automated Machine Learning (AutoML)\n",
       "- Automates model selection, hyperparameter optimization, and pipeline construction, democratizing AI for non‑experts.  \n",
       "- Techniques include neural architecture search (NAS), Bayesian optimization, and meta‑learning.  \n",
       "- Cloud‑native AutoML services (Google Vertex AI, Azure AutoML, AWS SageMaker Autopilot) are integrating with MLOps platforms to streamline end‑to‑end deployment.\n",
       "\n",
       "---\n",
       "\n",
       "## Challenges and Limitations\n",
       "\n",
       "### Data Bias  \n",
       "- **Source Imbalance**: Training datasets often over‑represent certain demographics, regions, or sensor modalities, leading to systematic performance degradation on under‑represented groups.  \n",
       "- **Label Noise**: Inconsistent or erroneous annotations propagate bias into model predictions, especially in supervised learning pipelines that lack robust cleaning mechanisms.  \n",
       "- **Feedback Loops**: Deployed models can reinforce existing biases by preferentially selecting data that matches prior predictions, creating a self‑fulfilling cycle.\n",
       "\n",
       "### Interpretability  \n",
       "- **Black‑Box Nature**: Deep neural networks and ensemble methods provide limited insight into decision pathways, hindering trust in high‑stakes domains such as healthcare and finance.  \n",
       "- **Post‑hoc Explanations**: Techniques like SHAP or LIME approximate feature importance but can be unstable across similar inputs, raising questions about their reliability.  \n",
       "- **Regulatory Pressure**: Emerging legal frameworks (e.g., GDPR, AI Act) demand transparent rationale for automated decisions, pressuring developers to incorporate explainable‑by‑design architectures.\n",
       "\n",
       "### Computational Cost  \n",
       "- **Training Resources**: State‑of‑the‑art models often require hundreds of GPU‑years, inflating carbon footprints and limiting accessibility for smaller organizations.  \n",
       "- **Inference Latency**: Real‑time applications (e.g., autonomous driving, fraud detection) demand sub‑millisecond response times, which large models struggle to meet without aggressive quantization or pruning.  \n",
       "- **Scalability**: Distributed training introduces communication overhead and synchronization bottlenecks, complicating efficient scaling across heterogeneous hardware.\n",
       "\n",
       "### Privacy Concerns  \n",
       "- **Data Leakage**: Models can inadvertently memorize sensitive training examples, enabling reconstruction attacks or membership inference attacks that expose private information.  \n",
       "- **Federated Learning Risks**: While decentralized training mitigates raw data sharing, gradient updates can still leak information, necessitating differential privacy or secure aggregation protocols.  \n",
       "- **Compliance**: Aligning model pipelines with regulations such as HIPAA, CCPA, or GDPR requires rigorous audit trails and data governance mechanisms.\n",
       "\n",
       "### Model Robustness  \n",
       "- **Adversarial Vulnerability**: Small, imperceptible perturbations can cause catastrophic misclassifications, highlighting the need for adversarial training or certified defenses.  \n",
       "- **Distribution Shift**: Models trained on static datasets often fail when confronted with evolving real‑world conditions (e.g., seasonal trends, sensor drift). Continuous monitoring and domain adaptation are essential.  \n",
       "- **Out‑of‑Domain Detection**: Absence of reliable uncertainty estimation leads to overconfident predictions on novel inputs, undermining safety‑critical deployments.  \n",
       "\n",
       "Collectively, these challenges underscore the necessity for interdisciplinary solutions that balance performance with ethical, legal, and operational considerations.\n",
       "\n",
       "---\n",
       "\n",
       "## Future Outlook\n",
       "\n",
       "### Emerging Research Directions\n",
       "- **Foundation Models for Multimodal Reasoning** – Next‑generation models will integrate vision, language, audio, and sensor data within a single architecture, enabling seamless cross‑modal understanding and generation.  \n",
       "- **Causal and Counterfactual Learning** – Researchers are shifting from correlation‑based prediction to causal inference, allowing systems to reason about “what‑if” scenarios and improve robustness to distribution shift.  \n",
       "- **Energy‑Efficient Training Paradigms** – Techniques such as sparsity‑induced training, neuromorphic hardware, and federated distillation aim to cut the carbon footprint of large‑scale model development by an order of magnitude.  \n",
       "- **Self‑Supervised Robotics** – By leveraging raw interaction streams, robots will acquire manipulation skills without explicit labeling, bridging the gap between simulation and real‑world deployment.  \n",
       "- **Trustworthy AI Frameworks** – Formal verification, interpretability toolkits, and privacy‑preserving learning (e.g., differential privacy, secure multiparty computation) will become integral components of the ML development pipeline.\n",
       "\n",
       "### Industry Adoption Trajectories\n",
       "- **Vertical Integration** – Sectors such as healthcare, finance, and manufacturing will embed domain‑specific ML pipelines directly into core operational systems, moving beyond pilot projects to become revenue‑generating services.  \n",
       "- **Edge‑Centric Intelligence** – With 5G/6G rollouts, inference will increasingly occur on devices (IoT nodes, autonomous vehicles, wearables), reducing latency and data‑transfer costs while enhancing privacy.  \n",
       "- **AI‑as‑a‑Service Maturation** – Cloud providers will offer modular, composable AI services (e.g., “explainability‑as‑a‑service” or “bias‑audit APIs”), lowering entry barriers for SMEs and accelerating time‑to‑value.  \n",
       "- **Regulatory‑Driven Compliance** – Anticipated standards (e.g., EU AI Act, US AI Bill of Rights) will compel firms to embed governance, audit trails, and risk‑assessment modules into their ML stacks, fostering a market for compliance‑focused tooling.  \n",
       "- **Human‑AI Collaboration Platforms** – Products that blend generative assistants with domain experts (e.g., AI‑augmented design tools, decision‑support dashboards) will dominate the enterprise software landscape, shifting the workforce toward higher‑level creative and strategic tasks.\n",
       "\n",
       "### Societal Impact Projections\n",
       "- **Labor Market Re‑skilling** – Automation of routine analytics and decision processes will displace certain roles while creating demand for AI‑fluency, data stewardship, and model‑maintenance expertise. Lifelong learning ecosystems will become essential.  \n",
       "- **Equity and Inclusion** – If bias mitigation research translates into practice, ML‑driven services can reduce systemic disparities (e.g., fair credit scoring, unbiased medical diagnostics). Conversely, neglecting these safeguards could exacerbate existing inequities.  \n",
       "- **Privacy Paradigm Shift** – Widespread adoption of privacy‑preserving ML will reshape expectations around personal data ownership, potentially leading to new legal frameworks and consumer empowerment tools.  \n",
       "- **Public Trust and Governance** – Transparent model reporting, open‑source audit tools, and participatory AI design processes will be critical to maintaining societal trust, especially in high‑stakes domains like criminal justice or public health.  \n",
       "- **Environmental Stewardship** – Energy‑aware ML research and hardware innovations will mitigate the ecological footprint of AI, aligning the technology’s growth with global sustainability goals.\n",
       "\n",
       "Collectively, these trends suggest a near‑term transition from experimental ML prototypes to deeply integrated, responsibly governed intelligence that reshapes both industry operations and everyday life.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "- **Key Takeaways**  \n",
       "  - Understanding the fundamentals of supervised, unsupervised, and reinforcement learning provides a solid foundation for tackling real‑world problems.  \n",
       "  - Mastery of data preprocessing, model selection, and evaluation metrics is essential to build robust, reliable systems.  \n",
       "  - Ethical considerations, interpretability, and scalability must be integrated into every stage of the machine‑learning pipeline.\n",
       "\n",
       "- **Why Mastery Matters**  \n",
       "  - Proficient ML practitioners can translate raw data into actionable insights, driving innovation across industries such as healthcare, finance, and transportation.  \n",
       "  - Deep knowledge reduces the risk of model bias, overfitting, and deployment failures, ensuring responsible and sustainable AI deployment.\n",
       "\n",
       "- **Recommendations for Further Study**  \n",
       "  1. **Advanced Topics** – Dive into deep learning architectures (CNNs, RNNs, Transformers), probabilistic graphical models, and meta‑learning.  \n",
       "  2. **Practical Experience** – Participate in Kaggle competitions, contribute to open‑source projects, and build end‑to‑end pipelines on cloud platforms.  \n",
       "  3. **Theoretical Foundations** – Study statistical learning theory, optimization algorithms, and information theory to strengthen intuition behind model behavior.  \n",
       "  4. **Ethics & Governance** – Explore frameworks for fairness, accountability, and transparency; stay updated on emerging regulations.  \n",
       "  5. **Continuous Learning** – Follow leading conferences (NeurIPS, ICML, CVPR), read recent arXiv papers, and engage with the ML community through webinars and meetups.\n",
       "\n",
       "By consolidating these concepts and pursuing the outlined pathways, learners can evolve from competent practitioners to thought leaders capable of shaping the future of machine learning.\n",
       "\n",
       "---\n",
       "\n",
       "## References\n",
       "\n",
       "### Books\n",
       "- **\"Artificial Intelligence: A Modern Approach\"** – Stuart Russell & Peter Norvig (3rd ed., 2020) – Comprehensive textbook covering AI fundamentals, search algorithms, machine learning, and ethics.  \n",
       "- **\"Deep Learning\"** – Ian Goodfellow, Yoshua Bengio & Aaron Courville (MIT Press, 2016) – In‑depth treatment of neural networks, optimization, and modern architectures.  \n",
       "- **\"Pattern Recognition and Machine Learning\"** – Christopher M. Bishop (Springer, 2006) – Classic reference on statistical pattern recognition and Bayesian methods.  \n",
       "- **\"Data Science for Business\"** – Foster Provost & Tom Fawcett (O'Reilly, 2013) – Bridges the gap between data‑driven techniques and business strategy.  \n",
       "- **\"Hands‑On Machine Learning with Scikit‑Learn, Keras, and TensorFlow\"** – Aurélien Géron (O'Reilly, 2022) – Practical guide with code examples for modern ML pipelines.\n",
       "\n",
       "### Research Papers\n",
       "- **LeCun, Y., Bengio, Y., & Hinton, G. (2015).** *Deep Learning.* **Nature, 521**, 436–444. DOI:10.1038/nature14539  \n",
       "- **Vaswani, A., et al. (2017).** *Attention Is All You Need.* **Advances in Neural Information Processing Systems, 30**, 5998‑6008. arXiv:1706.03762  \n",
       "- **Brown, T. B., et al. (2020).** *Language Models are Few‑Shot Learners.* **arXiv preprint arXiv:2005.14165**.  \n",
       "- **Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).** *ImageNet Classification with Deep Convolutional Neural Networks.* **Communications of the ACM, 60(6)**, 84‑90. DOI:10.1145/3065386  \n",
       "- **Raffel, C., et al. (2020).** *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.* **JMLR, 21(140)**, 1‑67. arXiv:1910.10683  \n",
       "\n",
       "### Online Courses & MOOCs\n",
       "- **[Machine Learning by Stanford University (Coursera)](https://www.coursera.org/learn/machine-learning)** – Andrew Ng’s foundational course covering supervised/unsupervised learning, neural networks, and best practices.  \n",
       "- **[Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning)** – Five‑course series (Neural Networks, CNNs, Sequence Models, etc.) by Andrew Ng and team.  \n",
       "- **[CS231n: Convolutional Neural Networks for Visual Recognition (Stanford)](http://cs231n.stanford.edu/)** – Lecture videos, notes, and assignments focusing on computer vision.  \n",
       "- **[Fast.ai Practical Deep Learning for Coders (free)](https://www.fast.ai/)** – Hands‑on approach using the fastai library and PyTorch.  \n",
       "- **[MIT 6.S191: Introduction to Deep Learning (MIT OpenCourseWare)](https://ocw.mit.edu/courses/6-s191-introduction-to-deep-learning-fall-2023/)** – Lecture videos, labs, and assignments with a research‑oriented perspective.\n",
       "\n",
       "### Reputable Websites & Resources\n",
       "- **[arXiv.org](https://arxiv.org/)** – Pre‑print repository for the latest papers in AI, ML, statistics, and related fields.  \n",
       "- **[Papers with Code](https://paperswithcode.com/)** – Links research papers to open‑source implementations and benchmark leaderboards.  \n",
       "- **[DeepLearning.AI Blog](https://www.deeplearning.ai/blog/)** – Articles, interviews, and updates from leading AI researchers.  \n",
       "- **[OpenAI Research](https://openai.com/research/)** – Publications and technical reports on large‑scale language models and safety.  \n",
       "- **[Google AI Blog](https://ai.googleblog.com/)** – Insights into cutting‑edge Google research, datasets, and tools.  \n",
       "\n",
       "These resources provide a solid foundation for further study, from theoretical underpinnings to practical implementation and current research trends."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = orchestrator_worker.invoke({\"topic\": \"crate a report on Machine Learning concpets\"})\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c8373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c3ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814122a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259153f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae7b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6584f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a37f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6daaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2b727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f92ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb5443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e19c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de1fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97467a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2c354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
