{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b8f7041",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Hello! How can I help you today?', additional_kwargs={'reasoning_content': 'We need to respond as ChatGPT. The user says \"hi\". Should respond friendly. No policy issues.'}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 72, 'total_tokens': 113, 'completion_time': 0.081474879, 'prompt_time': 0.002441754, 'queue_time': 0.046530765, 'total_time': 0.083916633}, 'model_name': 'openai/gpt-oss-120b', 'system_fingerprint': 'fp_82669fd91d', 'service_tier': 'on_demand', 'finish_reason': 'stop', 'logprobs': None}, id='run--dfe4396d-8686-4a4f-967f-4ccfaa7450ba-0', usage_metadata={'input_tokens': 72, 'output_tokens': 41, 'total_tokens': 113})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
    "groq_api_key = os.getenv(\"GROQ_API_KEY\")\n",
    "\n",
    "from langchain_groq import ChatGroq\n",
    "llm = ChatGroq(model=\"openai/gpt-oss-120b\", api_key=groq_api_key)\n",
    "llm.invoke(\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b910064",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing_extensions import Literal, TypedDict\n",
    "import operator\n",
    "from typing import Annotated, List\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.messages import HumanMessage, SystemMessage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e4f3393",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Section(BaseModel):\n",
    "    name:str=Field(description=\"Name for this section of the report\")\n",
    "    description:str=Field(description=\"Brief Overview of main topics and concepts of the section\")\n",
    "\n",
    "class Sections(BaseModel):\n",
    "    sections:List[Section]=Field(description=\"Sections of the report\")\n",
    "\n",
    "planner = llm.with_structured_output(Sections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49d2caa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, END, StateGraph\n",
    "from langgraph.types import Send\n",
    "\n",
    "class State(TypedDict):\n",
    "    topic:str\n",
    "    sections:list[Sections]\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]\n",
    "    final_report: str\n",
    "\n",
    "class WorkerState(TypedDict):\n",
    "    section:Section\n",
    "    completed_sections: Annotated[\n",
    "        list, operator.add\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4bbf932b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def orchestrator(state:State):\n",
    "    \"\"\"Orchestrator that generates a plan for the report\"\"\"\n",
    "    report_sections = planner.invoke(\n",
    "        \n",
    "           [ SystemMessage(content=\"Generate a plan for the report.\"),\n",
    "            HumanMessage(content=f\"Here is the report topic: {state[\"topic\"]}\"),]\n",
    "        \n",
    "    )\n",
    "    print(\"Report Section:\", report_sections)\n",
    "    return {\"sections\": report_sections.sections}\n",
    "\n",
    "def llm_call(state:WorkerState):\n",
    "    \"\"\"Worker writes a section of the report\"\"\"\n",
    "    section = llm.invoke(\n",
    "        \n",
    "            [SystemMessage(content=\"Write a report section following the provided name and description. Include no preamble for each section.use Markdown formatting.\"),\n",
    "            HumanMessage(content=f\"Here is the section name: {state['section'].name} and description: {state['section'].description}\"),\n",
    "        ]\n",
    "    )\n",
    "    return{\"completed_sections\": [section.content]}\n",
    "\n",
    "def assign_workers(state: State):\n",
    "    \"\"\" Assign a worker to each section in the plan\"\"\"\n",
    "    return [Send(\"llm_call\", {\"section\":s}) for s in state[\"sections\"]]\n",
    "\n",
    "def synthesizer(state: State):\n",
    "    \"\"\" Synthesize full report from sections\"\"\"\n",
    "    completed_sections = state[\"completed_sections\"]\n",
    "    completed_report_sections = \"\\n\\n---\\n\\n\".join(completed_sections)\n",
    "    return {\"final_report\": completed_report_sections}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bf37a81c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, START, END\n",
    "from IPython.display import display, Image\n",
    "\n",
    "orchestrator_worker_builder = StateGraph(State)\n",
    "orchestrator_worker_builder.add_node(\"orchestrator\", orchestrator)\n",
    "orchestrator_worker_builder.add_node(\"llm_call\", llm_call)\n",
    "orchestrator_worker_builder.add_node(\"synthesizer\", synthesizer)\n",
    "\n",
    "orchestrator_worker_builder.add_edge(START, \"orchestrator\")\n",
    "orchestrator_worker_builder.add_conditional_edges(\"orchestrator\", assign_workers, [\"llm_call\"])\n",
    "orchestrator_worker_builder.add_edge(\"llm_call\", \"synthesizer\")\n",
    "orchestrator_worker_builder.add_edge(\"synthesizer\", END)\n",
    "\n",
    "orchestrator_worker = orchestrator_worker_builder.compile()\n",
    "\n",
    "display(Image(orchestrator_worker.get_graph().draw_mermaid_png()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "21deee82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report Section: sections=[Section(name='Executive Summary', description='A concise overview of the report, highlighting the key machine learning concepts, their significance, and the main takeaways for readers.'), Section(name='Introduction to Machine Learning', description='Defines machine learning, its historical evolution, relationship to AI and statistics, and outlines the scope of the report.'), Section(name='Fundamental Concepts', description='Covers core ideas such as data representation, features, labels, training vs. testing, overfitting/underfitting, bias‚Äëvariance trade‚Äëoff, and the learning pipeline.'), Section(name='Supervised Learning', description='Explains the paradigm, key algorithms (linear regression, logistic regression, decision trees, SVM, k‚ÄëNN, neural networks), loss functions, and typical applications.'), Section(name='Unsupervised Learning', description='Describes clustering, dimensionality reduction, density estimation, and algorithms like k‚Äëmeans, hierarchical clustering, PCA, t‚ÄëSNE, and autoencoders.'), Section(name='Reinforcement Learning', description='Introduces agents, environments, reward signals, policies, value functions, and popular methods such as Q‚Äëlearning, SARSA, and deep RL (DQN, policy gradients).'), Section(name='Model Evaluation & Validation', description='Discusses metrics (accuracy, precision, recall, F1, ROC‚ÄëAUC, RMSE, etc.), cross‚Äëvalidation, confusion matrix, and model selection techniques.'), Section(name='Practical Considerations & Best Practices', description='Covers data preprocessing, feature engineering, hyperparameter tuning, regularization, scalability, and reproducibility.'), Section(name='Current Trends & Emerging Topics', description='Highlights deep learning, transfer learning, federated learning, explainable AI, ethical AI, and automated machine learning (AutoML).'), Section(name='Challenges and Limitations', description='Analyzes issues such as data bias, interpretability, computational cost, privacy concerns, and model robustness.'), Section(name='Future Outlook', description='Speculates on upcoming research directions, industry adoption, and societal impact of machine learning technologies.'), Section(name='Conclusion', description='Summarizes the main points, reiterates the importance of mastering machine‚Äëlearning concepts, and provides recommendations for further study.'), Section(name='References', description='A curated list of books, research papers, online courses, and reputable resources for deeper exploration.')]\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "## Executive Summary\n",
       "\n",
       "This report provides a concise overview of foundational and emerging machine‚Äëlearning concepts, emphasizing their practical significance and the strategic insights they deliver to organizations and practitioners.\n",
       "\n",
       "- **Core Concepts**  \n",
       "  - **Supervised Learning** ‚Äì Predictive modeling using labeled data (e.g., regression, classification).  \n",
       "  - **Unsupervised Learning** ‚Äì Discovering hidden structure without explicit labels (e.g., clustering, dimensionality reduction).  \n",
       "  - **Reinforcement Learning** ‚Äì Decision‚Äëmaking through interaction with an environment and reward feedback.  \n",
       "  - **Deep Learning** ‚Äì Hierarchical neural networks that excel in perception tasks such as image, speech, and language processing.  \n",
       "  - **Transfer & Few‚ÄëShot Learning** ‚Äì Leveraging pre‚Äëtrained models to adapt quickly to new tasks with minimal data.\n",
       "\n",
       "- **Significance**  \n",
       "  - Enables data‚Äëdriven automation, improving efficiency and reducing human error.  \n",
       "  - Drives competitive advantage by uncovering actionable insights from large, complex datasets.  \n",
       "  - Facilitates personalization at scale, enhancing customer experience across industries.  \n",
       "  - Supports real‚Äëtime decision making in critical domains (e.g., finance, healthcare, autonomous systems).\n",
       "\n",
       "- **Main Takeaways**  \n",
       "  1. **Strategic Alignment** ‚Äì Successful ML initiatives require clear business objectives, robust data pipelines, and cross‚Äëfunctional collaboration.  \n",
       "  2. **Model Selection & Evaluation** ‚Äì Choose algorithms that match the problem‚Äôs complexity, data availability, and interpretability needs; rigorously validate with appropriate metrics and cross‚Äëvalidation.  \n",
       "  3. **Ethics & Governance** ‚Äì Proactively address bias, privacy, and regulatory compliance to build trustworthy AI systems.  \n",
       "  4. **Scalability & Maintenance** ‚Äì Deploy models using containerization, CI/CD pipelines, and monitoring to ensure performance over time.  \n",
       "  5. **Continuous Learning** ‚Äì Stay abreast of rapid advances (e.g., foundation models, prompt engineering) to sustain innovation.\n",
       "\n",
       "Overall, mastering these concepts equips readers to design, implement, and govern machine‚Äëlearning solutions that deliver measurable value while navigating technical and ethical challenges.\n",
       "\n",
       "---\n",
       "\n",
       "## Introduction to Machine Learning\n",
       "\n",
       "Machine learning (ML) is a subfield of artificial intelligence that focuses on the development of algorithms and statistical models enabling computers to perform tasks without explicit programming. By learning patterns from data, ML systems can make predictions, classify information, and uncover insights that would be infeasible to encode manually.\n",
       "\n",
       "### Historical Evolution\n",
       "- **1950s‚Äì1960s:** Early concepts emerged with the perceptron and symbolic AI approaches, emphasizing rule‚Äëbased reasoning.\n",
       "- **1970s‚Äì1980s:** Introduction of decision trees, nearest‚Äëneighbor methods, and the back‚Äëpropagation algorithm for neural networks revived interest in data‚Äëdriven learning.\n",
       "- **1990s:** The rise of support vector machines, ensemble methods (e.g., boosting, bagging), and the proliferation of statistical learning theory formalized the field.\n",
       "- **2000s:** Availability of large datasets and increased computational power spurred the resurgence of deep neural networks, leading to breakthroughs in computer vision, speech, and natural language processing.\n",
       "- **2010s‚Äìpresent:** Integration of reinforcement learning, transfer learning, and generative models has expanded ML applications across industry, science, and everyday technology.\n",
       "\n",
       "### Relationship to AI and Statistics\n",
       "- **Artificial Intelligence:** ML is a core engine of modern AI, providing the learning capability that differentiates adaptive systems from static, rule‚Äëbased AI.\n",
       "- **Statistics:** ML shares many foundations with statistical inference‚Äîprobability theory, hypothesis testing, and model evaluation‚Äîbut places a stronger emphasis on prediction accuracy, scalability, and algorithmic efficiency.\n",
       "\n",
       "### Scope of This Report\n",
       "This report surveys the principal paradigms and techniques of machine learning, including:\n",
       "1. **Supervised Learning:** Regression, classification, and ensemble methods.\n",
       "2. **Unsupervised Learning:** Clustering, dimensionality reduction, and density estimation.\n",
       "3. **Reinforcement Learning:** Policy optimization and value‚Äëbased methods.\n",
       "4. **Deep Learning:** Convolutional, recurrent, and transformer architectures.\n",
       "5. **Practical Considerations:** Data preprocessing, model evaluation, interpretability, and ethical implications.\n",
       "\n",
       "By exploring these areas, the report aims to equip readers with a comprehensive understanding of how machine learning operates, its historical context, and its pivotal role within the broader AI ecosystem.\n",
       "\n",
       "---\n",
       "\n",
       "## Fundamental Concepts\n",
       "\n",
       "### Data Representation\n",
       "- **Raw Data**: The original observations collected from sensors, databases, or user inputs.\n",
       "- **Structured Format**: Typically organized into a matrix \\(X \\in \\mathbb{R}^{n \\times p}\\) where \\(n\\) is the number of samples and \\(p\\) the number of features.\n",
       "- **Numerical Encoding**: Categorical variables are transformed (e.g., one‚Äëhot, ordinal encoding) to enable mathematical operations.\n",
       "\n",
       "### Features and Labels\n",
       "- **Features (Inputs)**: The columns of \\(X\\); each captures a measurable attribute that may influence the target.\n",
       "- **Labels (Outputs/Targets)**: The vector \\(y \\in \\mathbb{R}^{n}\\) (regression) or \\(y \\in \\{0,1\\}^{n}\\) (classification) that the model aims to predict.\n",
       "- **Feature Engineering**: Process of creating, selecting, or transforming features to improve model expressiveness (e.g., scaling, interaction terms, dimensionality reduction).\n",
       "\n",
       "### Training vs. Testing\n",
       "| Phase      | Purpose                                    | Data Used                         |\n",
       "|------------|--------------------------------------------|-----------------------------------|\n",
       "| **Training** | Fit model parameters \\(\\theta\\) by minimizing a loss on known pairs \\((X_{\\text{train}}, y_{\\text{train}})\\). | Subset of data (typically 70‚Äë80‚ÄØ% of total). |\n",
       "| **Testing**  | Evaluate generalization performance on unseen data. | Held‚Äëout subset (typically 20‚Äë30‚ÄØ%). |\n",
       "- **Validation Set** (optional): Used for hyper‚Äëparameter tuning without contaminating the test set.\n",
       "\n",
       "### Overfitting & Underfitting\n",
       "- **Underfitting**: Model is too simple; high bias; poor performance on both training and test data.\n",
       "- **Overfitting**: Model captures noise; high variance; excellent training performance but degraded test performance.\n",
       "- **Symptoms**: Large gap between training and validation error indicates overfitting; both errors high indicate underfitting.\n",
       "\n",
       "### Bias‚ÄëVariance Trade‚Äëoff\n",
       "- **Bias**: Error introduced by approximating a real‚Äëworld problem with a simplified model.\n",
       "- **Variance**: Error due to sensitivity to fluctuations in the training set.\n",
       "- **Decomposition**: Expected squared error = \\(\\text{Bias}^2 + \\text{Variance} + \\text{Irreducible Noise}\\).\n",
       "- **Goal**: Choose model complexity that balances bias and variance to minimize total error.\n",
       "\n",
       "### Learning Pipeline\n",
       "1. **Data Collection** ‚Äì Gather raw observations from relevant sources.\n",
       "2. **Pre‚Äëprocessing** ‚Äì Clean, handle missing values, encode categorical variables, normalize/scale.\n",
       "3. **Feature Engineering** ‚Äì Create informative features, reduce dimensionality (PCA, feature selection).\n",
       "4. **Dataset Split** ‚Äì Partition into training, validation, and test sets (or use cross‚Äëvalidation).\n",
       "5. **Model Selection** ‚Äì Choose algorithmic family (linear, tree‚Äëbased, neural network, etc.).\n",
       "6. **Training** ‚Äì Optimize model parameters on the training set.\n",
       "7. **Hyper‚Äëparameter Tuning** ‚Äì Adjust settings (regularization strength, depth, learning rate) using validation data or automated search (grid/random/Bayesian).\n",
       "8. **Evaluation** ‚Äì Compute performance metrics (accuracy, RMSE, AUC, etc.) on the test set.\n",
       "9. **Deployment** ‚Äì Export the trained model, monitor real‚Äëworld performance, and trigger retraining when drift is detected.\n",
       "10. **Maintenance** ‚Äì Periodic re‚Äëevaluation, data updates, and model versioning.\n",
       "\n",
       "Understanding these foundational concepts equips practitioners to design robust machine‚Äëlearning solutions that generalize well beyond the data they were trained on.\n",
       "\n",
       "---\n",
       "\n",
       "## Supervised Learning\n",
       "\n",
       "Supervised learning is a machine‚Äëlearning paradigm in which a model is trained on a labeled dataset‚ÄØ‚Äî‚ÄØeach training example consists of an input vector **x** and a corresponding target label **y** (either a scalar, a vector, or a class). The goal is to learn a mapping *f(x;‚ÄØŒ∏)* parameterized by Œ∏ that can predict **y** for previously unseen inputs, minimizing a predefined loss function that quantifies prediction error.\n",
       "\n",
       "### Core Concepts\n",
       "| Element | Description |\n",
       "|---------|-------------|\n",
       "| **Training data** | Set ùîª = {(**x**·µ¢,‚ÄØ**y**·µ¢)}·µ¢‚Çå‚ÇÅ·¥∫, where **y**·µ¢ are known ground‚Äëtruth values. |\n",
       "| **Hypothesis space** | Family of functions (e.g., linear models, trees, neural nets) from which the learner selects the best candidate. |\n",
       "| **Loss function** | Measures discrepancy between predicted and true labels; the choice of loss drives the learning algorithm. |\n",
       "| **Optimization** | Adjusts parameters Œ∏ to minimize the empirical risk ùëÖÃÇ(Œ∏) = (1/N)‚àë‚Çô ‚Ñì(f(**x**‚Çô;Œ∏),‚ÄØ**y**‚Çô). |\n",
       "| **Generalization** | Ability of the learned model to perform well on new, unseen data, assessed via validation/test sets or cross‚Äëvalidation. |\n",
       "\n",
       "### Key Algorithms & Typical Losses\n",
       "\n",
       "| Algorithm | Typical Use‚ÄëCase | Model Form | Common Loss Functions |\n",
       "|-----------|------------------|------------|-----------------------|\n",
       "| **Linear Regression** | Predicting continuous outcomes (e.g., house prices) | **f(x)=w·µÄx + b** (affine) | Mean Squared Error (MSE): ‚Ñì = (y‚Äë≈∑)¬≤ |\n",
       "| **Logistic Regression** | Binary or multinomial classification (e.g., spam detection) | **f(x)=œÉ(w·µÄx + b)**, œÉ = sigmoid | Cross‚Äëentropy (log‚Äëloss): ‚Ñì = ‚àí[y¬∑log‚ÄØp + (1‚Äëy)¬∑log‚ÄØ(1‚Äëp)] |\n",
       "| **Decision Trees** | Interpretable classification/regression (e.g., credit scoring) | Hierarchical partition of feature space | Gini impurity / Entropy for classification; MSE for regression |\n",
       "| **Support Vector Machines (SVM)** | High‚Äëdimensional classification, margin‚Äëmaximizing tasks | Linear or kernel‚Äëinduced hyperplane | Hinge loss: ‚Ñì = max(0,‚ÄØ1‚Äëy¬∑f(x)) |\n",
       "| **k‚ÄëNearest Neighbors (k‚ÄëNN)** | Instance‚Äëbased learning where locality matters (e.g., recommendation) | Non‚Äëparametric, prediction by majority vote (classification) or average (regression) | No explicit loss; distance metric (e.g., Euclidean) drives prediction |\n",
       "| **Neural Networks (MLP, CNN, RNN, etc.)** | Complex patterns such as image, speech, and sequential data | Stacked layers of nonlinear transformations | MSE for regression, cross‚Äëentropy for classification, custom losses (e.g., CTC for speech) |\n",
       "\n",
       "#### Loss Function Highlights\n",
       "- **Mean Squared Error (MSE)** ‚Äì penalizes large deviations, smooth gradient, suited for regression.\n",
       "- **Mean Absolute Error (MAE)** ‚Äì robust to outliers, gradient is constant sign.\n",
       "- **Cross‚ÄëEntropy** ‚Äì measures divergence between true label distribution and predicted probabilities; encourages calibrated probabilities.\n",
       "- **Hinge Loss** ‚Äì focuses on margin violations; central to SVMs.\n",
       "- **Huber Loss** ‚Äì combines MSE (near zero error) and MAE (far from zero) for robustness.\n",
       "\n",
       "### Typical Applications\n",
       "- **Finance:** Credit risk scoring (logistic regression, decision trees), algorithmic trading signals (gradient‚Äëboosted trees, neural nets).\n",
       "- **Healthcare:** Disease diagnosis from imaging (CNNs), patient outcome prediction (random forests, survival models).\n",
       "- **Natural Language Processing:** Sentiment analysis (logistic regression, LSTM/Transformer models), spam filtering (SVM, Na√Øve Bayes).\n",
       "- **Computer Vision:** Object detection and classification (CNNs, ResNet, YOLO), image regression (depth estimation with fully‚Äëconnected nets).\n",
       "- **Recommender Systems:** Rating prediction (matrix factorization with supervised fine‚Äëtuning), click‚Äëthrough‚Äërate prediction (gradient‚Äëboosted decision trees).\n",
       "- **Manufacturing & IoT:** Predictive maintenance (random forests, gradient boosting), sensor fault detection (k‚ÄëNN, SVM).\n",
       "\n",
       "### Practical Workflow\n",
       "1. **Data collection & labeling** ‚Äì ensure high‚Äëquality, representative labels.\n",
       "2. **Pre‚Äëprocessing** ‚Äì handling missing values, feature scaling (standardization or min‚Äëmax), categorical encoding.\n",
       "3. **Model selection** ‚Äì choose algorithm(s) based on problem type, data size, interpretability needs.\n",
       "4. **Training** ‚Äì minimize the chosen loss via gradient‚Äëbased optimizers (SGD, Adam) or closed‚Äëform solutions (ordinary least squares for linear regression).\n",
       "5. **Validation** ‚Äì use cross‚Äëvalidation or a hold‚Äëout set to tune hyper‚Äëparameters (e.g., regularization Œª, tree depth, k in k‚ÄëNN).\n",
       "6. **Evaluation** ‚Äì appropriate metrics: RMSE / MAE for regression, accuracy / precision‚Äërecall / ROC‚ÄëAUC for classification.\n",
       "7. **Deployment & monitoring** ‚Äì export model (ONNX, TensorFlow SavedModel), monitor drift, and schedule periodic retraining.\n",
       "\n",
       "Supervised learning remains the workhorse of modern AI, providing a clear, data‚Äëdriven pathway from raw observations to actionable predictions across virtually every industry.\n",
       "\n",
       "---\n",
       "\n",
       "## Unsupervised Learning\n",
       "\n",
       "Unsupervised learning tackles tasks where the data lack explicit target labels. The goal is to uncover hidden structure, relationships, or distributions within the data. Core problem families include **clustering**, **dimensionality reduction**, and **density estimation**, each supported by a suite of algorithms.\n",
       "\n",
       "### 1. Clustering  \n",
       "Clustering partitions a dataset into groups (clusters) such that intra‚Äëcluster similarity is high and inter‚Äëcluster similarity is low.\n",
       "\n",
       "| Method | Principle | Typical Use‚ÄëCases |\n",
       "|--------|-----------|-------------------|\n",
       "| **k‚Äëmeans** | Minimizes within‚Äëcluster sum of squares (WCSS) by iteratively assigning points to the nearest centroid and updating centroids. | Large‚Äëscale segmentation, image quantization, market segmentation. |\n",
       "| **Hierarchical clustering** | Builds a dendrogram by either agglomerative (bottom‚Äëup) merging or divisive (top‚Äëdown) splitting based on a linkage criterion (single, complete, average, Ward). | Phylogenetic analysis, document taxonomy, exploratory data analysis where cluster hierarchy is valuable. |\n",
       "| **DBSCAN** (Density‚ÄëBased Spatial Clustering of Applications with Noise) | Groups points reachable within a radius Œµ with at least *minPts* neighbors; isolates low‚Äëdensity points as noise. | Spatial data, anomaly detection, data with irregular shapes. |\n",
       "| **Spectral clustering** | Constructs a similarity graph, computes the Laplacian eigenvectors, and runs k‚Äëmeans in the reduced spectral space. | Community detection in networks, image segmentation. |\n",
       "\n",
       "#### Practical considerations\n",
       "- **Choosing *k***: Elbow method, silhouette score, or information‚Äëtheoretic criteria (e.g., BIC) help select an appropriate number of clusters.\n",
       "- **Scaling**: Standardize or normalize features before distance‚Äëbased clustering to avoid dominance by high‚Äëvariance dimensions.\n",
       "- **Initialization**: k‚Äëmeans++ seeding reduces sensitivity to random starts.\n",
       "\n",
       "---\n",
       "\n",
       "### 2. Dimensionality Reduction  \n",
       "Dimensionality reduction transforms high‚Äëdimensional data into a lower‚Äëdimensional representation while preserving salient structure.\n",
       "\n",
       "| Technique | Objective | Key Characteristics |\n",
       "|-----------|-----------|----------------------|\n",
       "| **Principal Component Analysis (PCA)** | Linear projection onto orthogonal axes that maximize variance. Solved via eigen‚Äëdecomposition of the covariance matrix or singular value decomposition (SVD). | Fast, globally optimal for linear variance preservation, interpretable components. |\n",
       "| **t‚ÄëDistributed Stochastic Neighbor Embedding (t‚ÄëSNE)** | Non‚Äëlinear embedding that preserves local pairwise similarities by minimizing Kullback‚ÄëLeibler divergence between high‚Äëdimensional and low‚Äëdimensional similarity distributions. | Excellent for visualizing clusters; computationally intensive; perplexity hyperparameter controls neighborhood size. |\n",
       "| **Uniform Manifold Approximation and Projection (UMAP)** | Manifold learning based on fuzzy simplicial set construction; optimizes cross‚Äëentropy between high‚Äë and low‚Äëdimensional fuzzy topological representations. | Faster than t‚ÄëSNE, preserves both local and some global structure, scalable to millions of points. |\n",
       "| **Autoencoders** | Neural networks trained to reconstruct inputs; the bottleneck layer yields a compressed representation. Variants include variational autoencoders (VAEs) for probabilistic embeddings. | Captures non‚Äëlinear structure, can be tailored with convolutional or recurrent layers for specific data modalities. |\n",
       "\n",
       "#### Example: PCA derivation (brief)\n",
       "Given centered data matrix **X** ‚àà ‚Ñù^{n√ód}, the covariance matrix is **C** = (1/(n‚Äë1))‚ÄØ**X**·µÄ**X**. Eigen‚Äëdecomposition **C**‚ÄØ=‚ÄØ**UŒõU**·µÄ yields eigenvectors **U** (principal axes) and eigenvalues **Œõ** (explained variance). Projecting onto the first *p* eigenvectors gives the reduced data **Z** = **XU**_{:,1:p}.\n",
       "\n",
       "---\n",
       "\n",
       "### 3. Density Estimation  \n",
       "Density estimation models the underlying probability distribution *p(x)* of the data without supervision.\n",
       "\n",
       "| Approach | Description | When to Use |\n",
       "|----------|-------------|-------------|\n",
       "| **Parametric (e.g., Gaussian Mixture Models ‚Äì GMM)** | Assumes data arise from a mixture of *K* Gaussian components; parameters estimated via Expectation‚ÄëMaximization (EM). | When the data roughly follow multimodal Gaussian shapes and a closed‚Äëform likelihood is needed. |\n",
       "| **Non‚Äëparametric (e.g., Kernel Density Estimation ‚Äì KDE)** | Places a kernel (commonly Gaussian) on each data point and sums them to approximate *p(x)*. Bandwidth selection crucial. | Small‚Äëto‚Äëmoderate dimensional data where flexibility outweighs computational cost. |\n",
       "| **Neural density models (e.g., Normalizing Flows, VAEs)** | Learn invertible transformations or latent variable models to represent complex densities. | High‚Äëdimensional data (images, audio) where expressive, tractable likelihoods are required. |\n",
       "\n",
       "#### GMM EM steps (summary)\n",
       "1. **E‚Äëstep**: Compute responsibilities Œ≥_{ik} = \\frac{œÄ_k‚ÄØùí©(x_i‚ÄØ|‚ÄØŒº_k, Œ£_k)}{\\sum_{j=1}^{K} œÄ_j‚ÄØùí©(x_i‚ÄØ|‚ÄØŒº_j, Œ£_j)}.  \n",
       "2. **M‚Äëstep**: Update parameters  \n",
       "   - œÄ_k = (1/n)‚ÄØ‚àë_i Œ≥_{ik}  \n",
       "   - Œº_k = \\frac{\\sum_i Œ≥_{ik} x_i}{\\sum_i Œ≥_{ik}}  \n",
       "   - Œ£_k = \\frac{\\sum_i Œ≥_{ik} (x_i‚ÄëŒº_k)(x_i‚ÄëŒº_k)·µÄ}{\\sum_i Œ≥_{ik}}.\n",
       "\n",
       "---\n",
       "\n",
       "### 4. Representative Algorithms in Practice\n",
       "\n",
       "#### k‚Äëmeans (pseudo‚Äëcode)\n",
       "```python\n",
       "def kmeans(X, k, max_iter=300):\n",
       "    centroids = init_centroids(X, k)          # e.g., k‚Äëmeans++\n",
       "    for _ in range(max_iter):\n",
       "        # Assignment step\n",
       "        labels = np.argmin(distances(X, centroids), axis=1)\n",
       "        # Update step\n",
       "        new_centroids = np.array([X[labels==j].mean(axis=0) for j in range(k)])\n",
       "        if np.allclose(centroids, new_centroids):\n",
       "            break\n",
       "        centroids = new_centroids\n",
       "    return labels, centroids\n",
       "```\n",
       "\n",
       "#### Hierarchical agglomerative clustering (linkage)\n",
       "```python\n",
       "from scipy.cluster.hierarchy import linkage, fcluster\n",
       "Z = linkage(X, method='ward')          # builds dendrogram\n",
       "clusters = fcluster(Z, t=5, criterion='maxclust')\n",
       "```\n",
       "\n",
       "#### Autoencoder (Keras sketch)\n",
       "```python\n",
       "input_dim = X.shape[1]\n",
       "latent_dim = 32\n",
       "\n",
       "input_layer = Input(shape=(input_dim,))\n",
       "encoded = Dense(128, activation='relu')(input_layer)\n",
       "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
       "\n",
       "decoded = Dense(128, activation='relu')(encoded)\n",
       "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
       "\n",
       "autoencoder = Model(input_layer, decoded)\n",
       "autoencoder.compile(optimizer='adam', loss='mse')\n",
       "autoencoder.fit(X, X, epochs=50, batch_size=256, shuffle=True)\n",
       "```\n",
       "\n",
       "---\n",
       "\n",
       "### 5. Choosing the Right Unsupervised Technique\n",
       "\n",
       "| Data characteristic | Recommended approach |\n",
       "|---------------------|----------------------|\n",
       "| **Clear, spherical clusters, low‚Äëdimensional** | k‚Äëmeans (with careful *k* selection) |\n",
       "| **Irregularly shaped clusters or noise** | DBSCAN or hierarchical clustering |\n",
       "| **Need for a hierarchy of groups** | Agglomerative clustering with dendrogram analysis |\n",
       "| **Very high dimensional, linear structure** | PCA (or randomized SVD for scalability) |\n",
       "| **Visualization of complex manifolds** | t‚ÄëSNE or UMAP (tune perplexity / n_neighbors) |\n",
       "| **Non‚Äëlinear feature extraction** | Autoencoders / Variational Autoencoders |\n",
       "| **Probabilistic modeling of multimodal data** | Gaussian Mixture Models or Normalizing Flows |\n",
       "\n",
       "---\n",
       "\n",
       "### 6. Common Pitfalls & Best Practices\n",
       "\n",
       "1. **Feature scaling** ‚Äì Distance‚Äëbased methods (k‚Äëmeans, hierarchical) are highly sensitive to scale; always standardize or normalize.\n",
       "2. **Curse of dimensionality** ‚Äì In high dimensions, distances become less discriminative; apply dimensionality reduction before clustering.\n",
       "3. **Hyperparameter sensitivity** ‚Äì Algorithms like DBSCAN (Œµ, minPts) and t‚ÄëSNE (perplexity, learning rate) require systematic tuning, often via grid search or visual inspection.\n",
       "4. **Interpretability vs. performance** ‚Äì Linear methods (PCA) are easier to interpret, while deep autoencoders may yield better compression at the cost of opacity.\n",
       "5. **Evaluation without labels** ‚Äì Use internal metrics (silhouette score, Davies‚ÄëBouldin index) and stability analysis (e.g., bootstrapping) to assess clustering quality.\n",
       "\n",
       "---\n",
       "\n",
       "By mastering these unsupervised techniques, practitioners can extract meaningful patterns, reduce computational burden, and lay the groundwork for downstream supervised tasks or exploratory insights.\n",
       "\n",
       "---\n",
       "\n",
       "## Reinforcement Learning\n",
       "\n",
       "Reinforcement Learning (RL) studies how an **agent** interacts with an **environment** to achieve a goal defined by a **reward signal**. At each time step *t*, the agent observes a state *s‚Çú*, selects an action *a‚Çú* according to its **policy** œÄ, receives a scalar reward *r‚Çú‚Çä‚ÇÅ*, and transitions to a new state *s‚Çú‚Çä‚ÇÅ*. The objective is to learn a policy that maximizes the expected cumulative discounted return  \n",
       "\n",
       "\\[\n",
       "G_t = \\sum_{k=0}^{\\infty} \\gamma^{k} r_{t+k+1},\n",
       "\\]\n",
       "\n",
       "where Œ≥ ‚àà [0,1] is the discount factor.\n",
       "\n",
       "### Core Concepts  \n",
       "\n",
       "| Concept | Definition |\n",
       "|---------|------------|\n",
       "| **Agent** | The decision‚Äëmaking entity that selects actions. |\n",
       "| **Environment** | The external system with which the agent interacts; it supplies states and rewards. |\n",
       "| **Reward Signal** | Immediate scalar feedback indicating the desirability of a transition. |\n",
       "| **Policy (œÄ)** | Mapping from states to a probability distribution over actions (œÄ(a|s)). Can be deterministic (a = Œº(s)) or stochastic. |\n",
       "| **Value Functions** | Quantify expected return: <br>‚Ä¢ **State‚Äëvalue** VœÄ(s) = ùîºœÄ[G‚Çú | s‚Çú = s] <br>‚Ä¢ **Action‚Äëvalue** QœÄ(s,a) = ùîºœÄ[G‚Çú | s‚Çú = s, a‚Çú = a]. |\n",
       "| **Model** (optional) | A representation of the environment dynamics *p(s' , r | s, a)* used for planning. |\n",
       "\n",
       "### Classical Tabular Methods  \n",
       "\n",
       "1. **Q‚ÄëLearning** (off‚Äëpolicy)  \n",
       "   \\[\n",
       "   Q(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\big]\n",
       "   \\]  \n",
       "   - Learns the optimal action‚Äëvalue function *Q\\** independent of the behavior policy.  \n",
       "   - Converges to *Q\\** under appropriate learning‚Äërate schedules and sufficient exploration.\n",
       "\n",
       "2. **SARSA** (on‚Äëpolicy)  \n",
       "   \\[\n",
       "   Q(s,a) \\leftarrow Q(s,a) + \\alpha \\big[ r + \\gamma Q(s',a') - Q(s,a) \\big]\n",
       "   \\]  \n",
       "   - Updates using the action *a'* actually taken by the current policy, preserving the on‚Äëpolicy nature.  \n",
       "\n",
       "Both algorithms can be combined with **Œµ‚Äëgreedy** exploration, where the agent selects a random action with probability Œµ and the greedy action otherwise.\n",
       "\n",
       "### Deep Reinforcement Learning  \n",
       "\n",
       "When the state or action space is large or continuous, tabular representations become infeasible. **Deep RL** employs neural networks as function approximators for value functions or policies.\n",
       "\n",
       "#### Deep Q‚ÄëNetwork (DQN)  \n",
       "\n",
       "- Approximates the action‚Äëvalue function with a deep network *Q(s,a; Œ∏)*.  \n",
       "- Key techniques:  \n",
       "  - **Experience Replay**: Stores transitions in a buffer and samples mini‚Äëbatches to break correlation.  \n",
       "  - **Target Network**: A separate network *Œ∏‚Åª* provides stable target values, updated periodically.  \n",
       "- Update rule (mini‚Äëbatch loss):  \n",
       "  \\[\n",
       "  L(\\theta) = \\mathbb{E}_{(s,a,r,s')}\\Big[ \\big(r + \\gamma \\max_{a'} Q(s',a';\\theta^{-}) - Q(s,a;\\theta) \\big)^2 \\Big].\n",
       "  \\]\n",
       "\n",
       "#### Policy Gradient Methods  \n",
       "\n",
       "Instead of estimating value functions, these methods directly optimize the policy parameters Œ∏ by gradient ascent on the expected return:\n",
       "\n",
       "\\[\n",
       "\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}\\big[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, G_t \\big].\n",
       "\\]\n",
       "\n",
       "Common variants:\n",
       "\n",
       "- **REINFORCE** (Monte‚ÄëCarlo policy gradient) ‚Äì uses full episode returns as the advantage estimator.  \n",
       "- **Actor‚ÄëCritic** ‚Äì combines a policy (actor) with a learned value function (critic) to reduce variance:  \n",
       "  \\[\n",
       "  \\nabla_\\theta J \\approx \\mathbb{E}\\big[ \\nabla_\\theta \\log \\pi_\\theta(a|s) \\, (r + \\gamma V_w(s') - V_w(s)) \\big].\n",
       "  \\]  \n",
       "\n",
       "#### Advanced Deep RL Algorithms  \n",
       "\n",
       "| Algorithm | Core Idea |\n",
       "|-----------|-----------|\n",
       "| **DDPG** (Deep Deterministic Policy Gradient) | Extends policy gradients to continuous actions using an actor‚Äëcritic with deterministic policies. |\n",
       "| **A3C / A2C** (Asynchronous Advantage Actor‚ÄëCritic) | Parallel workers collect experience; the advantage function A(s,a) = Q(s,a) ‚Äì V(s) stabilizes learning. |\n",
       "| **PPO** (Proximal Policy Optimization) | Clips policy updates to stay within a trust region, offering a simple yet robust alternative to TRPO. |\n",
       "| **SAC** (Soft Actor‚ÄëCritic) | Maximizes a trade‚Äëoff between expected return and entropy, encouraging exploration. |\n",
       "\n",
       "### Summary  \n",
       "\n",
       "Reinforcement Learning provides a unified framework for sequential decision making. Starting from simple tabular algorithms such as Q‚Äëlearning and SARSA, the field has progressed to deep architectures that can handle high‚Äëdimensional perceptual inputs and continuous control tasks. The central pillars‚Äîagents, environments, reward signals, policies, and value functions‚Äîremain the same, while modern methods enrich them with function approximation, stability tricks, and sophisticated policy‚Äëoptimization techniques.\n",
       "\n",
       "---\n",
       "\n",
       "## Model Evaluation & Validation\n",
       "\n",
       "### Key Performance Metrics  \n",
       "\n",
       "| Metric | Typical Use‚ÄëCase | Interpretation |\n",
       "|--------|------------------|----------------|\n",
       "| **Accuracy** | Balanced classification problems | Proportion of correctly classified instances. |\n",
       "| **Precision** | Scenarios where false positives are costly (e.g., spam detection) | Ratio of true positives to all predicted positives. |\n",
       "| **Recall (Sensitivity)** | Situations where missing a positive is critical (e.g., disease screening) | Ratio of true positives to all actual positives. |\n",
       "| **F1‚ÄëScore** | Imbalanced classification where both precision and recall matter | Harmonic mean of precision and recall; balances the two. |\n",
       "| **ROC‚ÄëAUC** | Binary classifiers with varying decision thresholds | Area under the Receiver Operating Characteristic curve; measures discriminative ability across thresholds. |\n",
       "| **RMSE (Root Mean Squared Error)** | Regression tasks | Square root of average squared prediction error; penalizes large deviations. |\n",
       "| **MAE (Mean Absolute Error)** | Regression tasks where outliers are less critical | Average absolute difference between predictions and true values. |\n",
       "| **R¬≤ (Coefficient of Determination)** | Regression model goodness‚Äëof‚Äëfit | Proportion of variance in the target explained by the model. |\n",
       "\n",
       "### Confusion Matrix  \n",
       "\n",
       "A confusion matrix provides a granular view of classification outcomes:\n",
       "\n",
       "|                | **Predicted Positive** | **Predicted Negative** |\n",
       "|----------------|------------------------|------------------------|\n",
       "| **Actual Positive** | True Positive (TP) | False Negative (FN) |\n",
       "| **Actual Negative** | False Positive (FP) | True Negative (TN) |\n",
       "\n",
       "From the matrix we derive precision, recall, specificity, and other derived metrics. Visualizing the matrix (heatmap) helps spot systematic misclassifications.\n",
       "\n",
       "### Cross‚ÄëValidation Strategies  \n",
       "\n",
       "| Strategy | Description | When to Use |\n",
       "|----------|-------------|-------------|\n",
       "| **k‚ÄëFold CV** | Data split into *k* equally sized folds; each fold serves as validation once. | General purpose; balances bias‚Äëvariance trade‚Äëoff. |\n",
       "| **Stratified k‚ÄëFold** | Maintains class distribution within each fold. | Classification with imbalanced classes. |\n",
       "| **Leave‚ÄëOne‚ÄëOut (LOO)** | Each observation is a validation set. | Very small datasets; high variance acceptable. |\n",
       "| **Time‚ÄëSeries CV (Rolling/Expanding Window)** | Preserves temporal order; training set grows forward in time. | Forecasting or any sequential data. |\n",
       "| **Nested CV** | Outer loop for model assessment, inner loop for hyper‚Äëparameter tuning. | Prevents optimistic bias when tuning many hyper‚Äëparameters. |\n",
       "\n",
       "**Practical tip:** Report the mean and standard deviation of the chosen metric across folds to convey stability.\n",
       "\n",
       "### Model Selection Techniques  \n",
       "\n",
       "1. **Grid Search** ‚Äì Exhaustive exploration of a predefined hyper‚Äëparameter grid.  \n",
       "2. **Random Search** ‚Äì Random sampling of hyper‚Äëparameter space; often more efficient for high‚Äëdimensional spaces.  \n",
       "3. **Bayesian Optimization** ‚Äì Probabilistic model (e.g., Gaussian Process) guides the search toward promising regions.  \n",
       "4. **Successive Halving / Hyperband** ‚Äì Allocates resources adaptively, discarding poor configurations early.  \n",
       "5. **Ensemble Methods** ‚Äì Combine multiple models (bagging, boosting, stacking) to improve predictive performance and robustness.  \n",
       "\n",
       "**Selection criteria:**  \n",
       "- Primary metric aligned with business objective (e.g., maximize recall for safety‚Äëcritical alerts).  \n",
       "- Model complexity vs. interpretability trade‚Äëoff.  \n",
       "- Computational cost and latency constraints for deployment.  \n",
       "\n",
       "### Validation Workflow Summary  \n",
       "\n",
       "1. **Define target metric(s)** based on problem requirements.  \n",
       "2. **Partition data** using an appropriate CV scheme (stratified k‚Äëfold for classification, time‚Äëseries CV for temporal data).  \n",
       "3. **Train candidate models** with hyper‚Äëparameter tuning (grid/random/Bayesian).  \n",
       "4. **Evaluate** each candidate on validation folds; capture mean ¬± std of metrics, confusion matrices, and ROC curves.  \n",
       "5. **Select** the model that best balances performance, stability, and operational constraints.  \n",
       "6. **Perform final assessment** on an untouched test set (or via nested CV) to obtain an unbiased estimate of real‚Äëworld performance.  \n",
       "\n",
       "By systematically applying these metrics, validation strategies, and selection techniques, we ensure that the chosen model is both statistically sound and aligned with the project‚Äôs practical goals.\n",
       "\n",
       "---\n",
       "\n",
       "## Practical Considerations & Best Practices\n",
       "\n",
       "### 1. Data Preprocessing\n",
       "- **Data Cleaning**  \n",
       "  - Remove or impute missing values (e.g., median imputation for numeric features, most‚Äëfrequent for categorical).  \n",
       "  - Detect and handle outliers using robust statistics (IQR, MAD) or domain‚Äëspecific thresholds.  \n",
       "- **Normalization & Scaling**  \n",
       "  - Apply standard scaling (zero mean, unit variance) for algorithms sensitive to feature magnitude (e.g., SVM, neural nets).  \n",
       "  - Use min‚Äëmax scaling for bounded inputs or when preserving zero values is important.  \n",
       "- **Encoding Categorical Variables**  \n",
       "  - One‚Äëhot encode low‚Äëcardinality variables; use target encoding or embeddings for high‚Äëcardinality features.  \n",
       "- **Temporal & Leakage Checks**  \n",
       "  - Ensure training data precedes validation/test data in time‚Äëseries contexts.  \n",
       "  - Exclude features that leak future information (e.g., post‚Äëevent indicators).  \n",
       "\n",
       "### 2. Feature Engineering\n",
       "- **Domain‚ÄëDriven Features**  \n",
       "  - Leverage domain knowledge to create interaction terms, ratios, or domain‚Äëspecific aggregates.  \n",
       "- **Automated Feature Construction**  \n",
       "  - Use tools like FeatureTools or polynomial feature generators for systematic expansion.  \n",
       "- **Dimensionality Reduction**  \n",
       "  - Apply PCA, t‚ÄëSNE, or autoencoders when the feature space is extremely high‚Äëdimensional.  \n",
       "- **Feature Selection**  \n",
       "  - Filter methods (mutual information, chi‚Äësquare), wrapper methods (recursive feature elimination), or model‚Äëbased importance (tree SHAP).  \n",
       "\n",
       "### 3. Hyperparameter Tuning\n",
       "- **Search Strategies**  \n",
       "  - **Grid Search** for small, well‚Äëunderstood parameter spaces.  \n",
       "  - **Random Search** for larger spaces; often yields near‚Äëoptimal results faster.  \n",
       "  - **Bayesian Optimization** (e.g., Optuna, Hyperopt) for efficient exploration of complex spaces.  \n",
       "- **Cross‚ÄëValidation Scheme**  \n",
       "  - Use stratified k‚Äëfold for classification, time‚Äëseries split for sequential data.  \n",
       "  - Keep validation folds consistent across tuning runs to reduce variance.  \n",
       "- **Early Stopping & Warm‚ÄëStarts**  \n",
       "  - Integrate early stopping criteria for iterative models (GBMs, neural nets) to avoid over‚Äëtraining.  \n",
       "  - Warm‚Äëstart models when expanding hyperparameter grids to reuse previous fits.  \n",
       "\n",
       "### 4. Regularization\n",
       "- **L1 & L2 Penalties**  \n",
       "  - L1 (Lasso) for sparsity; L2 (Ridge) for coefficient shrinkage.  \n",
       "  - Elastic Net combines both, balancing sparsity and stability.  \n",
       "- **Dropout & Batch Normalization** (Neural Networks)  \n",
       "  - Dropout mitigates co‚Äëadaptation of neurons; batch norm stabilizes training dynamics.  \n",
       "- **Tree‚ÄëBased Model Controls**  \n",
       "  - Limit tree depth, set minimum child weight, use subsampling of rows/columns to reduce over‚Äëfitting.  \n",
       "\n",
       "### 5. Scalability\n",
       "- **Data Partitioning**  \n",
       "  - Store data in columnar formats (Parquet, ORC) and leverage lazy loading (Dask, Spark).  \n",
       "- **Distributed Training**  \n",
       "  - Use frameworks like Spark MLlib, Horovod, or Ray Tune for parallel hyperparameter search.  \n",
       "- **Model Compression**  \n",
       "  - Apply quantization, pruning, or knowledge distillation for inference on resource‚Äëconstrained environments.  \n",
       "\n",
       "### 6. Reproducibility\n",
       "- **Version Control**  \n",
       "  - Track code, configuration files, and model artifacts with Git and DVC or MLflow.  \n",
       "- **Random Seed Management**  \n",
       "  - Set seeds for all libraries (NumPy, TensorFlow/PyTorch, scikit‚Äëlearn) and document them.  \n",
       "- **Environment Specification**  \n",
       "  - Capture dependencies via `requirements.txt`, `environment.yml`, or Docker containers.  \n",
       "- **Experiment Logging**  \n",
       "  - Log metrics, hyperparameters, and data versions to a central store (e.g., MLflow, Weights & Biases).  \n",
       "- **Automated Pipelines**  \n",
       "  - Encode the entire workflow (preprocessing ‚Üí training ‚Üí evaluation) in reproducible pipelines (Airflow, Prefect, Kubeflow).  \n",
       "\n",
       "By systematically addressing these practical aspects, teams can build models that are not only performant but also robust, scalable, and maintainable over the long term.\n",
       "\n",
       "---\n",
       "\n",
       "## Current Trends & Emerging Topics\n",
       "\n",
       "### Deep Learning\n",
       "- Continues to dominate AI research and applications, driven by larger model architectures (e.g., transformers) and ever‚Äëincreasing compute resources.  \n",
       "- Advances in model scaling (GPT‚Äë4, PaLM, LLaMA) demonstrate emergent capabilities such as few‚Äëshot learning and reasoning.  \n",
       "- Specialized hardware (TPUs, GPUs, AI accelerators) and software stacks (CUDA, ROCm, JAX) accelerate training and inference pipelines.\n",
       "\n",
       "### Transfer Learning\n",
       "- Pre‚Äëtrained models are fine‚Äëtuned on domain‚Äëspecific data, reducing the need for massive labeled datasets.  \n",
       "- Techniques such as prompt‚Äëtuning, adapters, and LoRA (Low‚ÄëRank Adaptation) enable rapid customization while preserving core knowledge.  \n",
       "- Transfer learning is expanding beyond NLP to vision (ViT‚Äëbased models), speech, and multimodal domains.\n",
       "\n",
       "### Federated Learning\n",
       "- Enables collaborative model training across decentralized devices while keeping raw data local, addressing privacy and regulatory constraints.  \n",
       "- Recent work focuses on communication efficiency (compression, sparsification), robustness to non‚ÄëIID data, and secure aggregation protocols.  \n",
       "- Real‚Äëworld deployments are emerging in mobile keyboards, healthcare networks, and IoT edge ecosystems.\n",
       "\n",
       "### Explainable AI (XAI)\n",
       "- Provides transparency into black‚Äëbox models through post‚Äëhoc methods (SHAP, LIME) and intrinsically interpretable architectures (attention maps, prototype networks).  \n",
       "- Regulatory pressure (e.g., EU AI Act) and industry demand for auditability are pushing XAI from research labs into production systems.  \n",
       "- Emerging standards aim to quantify explanation fidelity, user trust, and actionable insights.\n",
       "\n",
       "### Ethical AI\n",
       "- Addresses bias mitigation, fairness, accountability, and societal impact throughout the AI lifecycle.  \n",
       "- Frameworks such as AI Ethics Guidelines, Model Cards, and Datasheets for Datasets promote responsible development and documentation.  \n",
       "- Ongoing research explores algorithmic fairness metrics, causal analysis for bias detection, and governance mechanisms for AI oversight.\n",
       "\n",
       "### Automated Machine Learning (AutoML)\n",
       "- Automates model selection, hyperparameter optimization, and pipeline construction, democratizing AI for non‚Äëexperts.  \n",
       "- Techniques include neural architecture search (NAS), Bayesian optimization, and meta‚Äëlearning.  \n",
       "- Cloud‚Äënative AutoML services (Google Vertex AI, Azure AutoML, AWS SageMaker Autopilot) are integrating with MLOps platforms to streamline end‚Äëto‚Äëend deployment.\n",
       "\n",
       "---\n",
       "\n",
       "## Challenges and Limitations\n",
       "\n",
       "### Data Bias  \n",
       "- **Source Imbalance**: Training datasets often over‚Äërepresent certain demographics, regions, or sensor modalities, leading to systematic performance degradation on under‚Äërepresented groups.  \n",
       "- **Label Noise**: Inconsistent or erroneous annotations propagate bias into model predictions, especially in supervised learning pipelines that lack robust cleaning mechanisms.  \n",
       "- **Feedback Loops**: Deployed models can reinforce existing biases by preferentially selecting data that matches prior predictions, creating a self‚Äëfulfilling cycle.\n",
       "\n",
       "### Interpretability  \n",
       "- **Black‚ÄëBox Nature**: Deep neural networks and ensemble methods provide limited insight into decision pathways, hindering trust in high‚Äëstakes domains such as healthcare and finance.  \n",
       "- **Post‚Äëhoc Explanations**: Techniques like SHAP or LIME approximate feature importance but can be unstable across similar inputs, raising questions about their reliability.  \n",
       "- **Regulatory Pressure**: Emerging legal frameworks (e.g., GDPR, AI Act) demand transparent rationale for automated decisions, pressuring developers to incorporate explainable‚Äëby‚Äëdesign architectures.\n",
       "\n",
       "### Computational Cost  \n",
       "- **Training Resources**: State‚Äëof‚Äëthe‚Äëart models often require hundreds of GPU‚Äëyears, inflating carbon footprints and limiting accessibility for smaller organizations.  \n",
       "- **Inference Latency**: Real‚Äëtime applications (e.g., autonomous driving, fraud detection) demand sub‚Äëmillisecond response times, which large models struggle to meet without aggressive quantization or pruning.  \n",
       "- **Scalability**: Distributed training introduces communication overhead and synchronization bottlenecks, complicating efficient scaling across heterogeneous hardware.\n",
       "\n",
       "### Privacy Concerns  \n",
       "- **Data Leakage**: Models can inadvertently memorize sensitive training examples, enabling reconstruction attacks or membership inference attacks that expose private information.  \n",
       "- **Federated Learning Risks**: While decentralized training mitigates raw data sharing, gradient updates can still leak information, necessitating differential privacy or secure aggregation protocols.  \n",
       "- **Compliance**: Aligning model pipelines with regulations such as HIPAA, CCPA, or GDPR requires rigorous audit trails and data governance mechanisms.\n",
       "\n",
       "### Model Robustness  \n",
       "- **Adversarial Vulnerability**: Small, imperceptible perturbations can cause catastrophic misclassifications, highlighting the need for adversarial training or certified defenses.  \n",
       "- **Distribution Shift**: Models trained on static datasets often fail when confronted with evolving real‚Äëworld conditions (e.g., seasonal trends, sensor drift). Continuous monitoring and domain adaptation are essential.  \n",
       "- **Out‚Äëof‚ÄëDomain Detection**: Absence of reliable uncertainty estimation leads to overconfident predictions on novel inputs, undermining safety‚Äëcritical deployments.  \n",
       "\n",
       "Collectively, these challenges underscore the necessity for interdisciplinary solutions that balance performance with ethical, legal, and operational considerations.\n",
       "\n",
       "---\n",
       "\n",
       "## Future Outlook\n",
       "\n",
       "### Emerging Research Directions\n",
       "- **Foundation Models for Multimodal Reasoning** ‚Äì Next‚Äëgeneration models will integrate vision, language, audio, and sensor data within a single architecture, enabling seamless cross‚Äëmodal understanding and generation.  \n",
       "- **Causal and Counterfactual Learning** ‚Äì Researchers are shifting from correlation‚Äëbased prediction to causal inference, allowing systems to reason about ‚Äúwhat‚Äëif‚Äù scenarios and improve robustness to distribution shift.  \n",
       "- **Energy‚ÄëEfficient Training Paradigms** ‚Äì Techniques such as sparsity‚Äëinduced training, neuromorphic hardware, and federated distillation aim to cut the carbon footprint of large‚Äëscale model development by an order of magnitude.  \n",
       "- **Self‚ÄëSupervised Robotics** ‚Äì By leveraging raw interaction streams, robots will acquire manipulation skills without explicit labeling, bridging the gap between simulation and real‚Äëworld deployment.  \n",
       "- **Trustworthy AI Frameworks** ‚Äì Formal verification, interpretability toolkits, and privacy‚Äëpreserving learning (e.g., differential privacy, secure multiparty computation) will become integral components of the ML development pipeline.\n",
       "\n",
       "### Industry Adoption Trajectories\n",
       "- **Vertical Integration** ‚Äì Sectors such as healthcare, finance, and manufacturing will embed domain‚Äëspecific ML pipelines directly into core operational systems, moving beyond pilot projects to become revenue‚Äëgenerating services.  \n",
       "- **Edge‚ÄëCentric Intelligence** ‚Äì With 5G/6G rollouts, inference will increasingly occur on devices (IoT nodes, autonomous vehicles, wearables), reducing latency and data‚Äëtransfer costs while enhancing privacy.  \n",
       "- **AI‚Äëas‚Äëa‚ÄëService Maturation** ‚Äì Cloud providers will offer modular, composable AI services (e.g., ‚Äúexplainability‚Äëas‚Äëa‚Äëservice‚Äù or ‚Äúbias‚Äëaudit APIs‚Äù), lowering entry barriers for SMEs and accelerating time‚Äëto‚Äëvalue.  \n",
       "- **Regulatory‚ÄëDriven Compliance** ‚Äì Anticipated standards (e.g., EU AI Act, US AI Bill of Rights) will compel firms to embed governance, audit trails, and risk‚Äëassessment modules into their ML stacks, fostering a market for compliance‚Äëfocused tooling.  \n",
       "- **Human‚ÄëAI Collaboration Platforms** ‚Äì Products that blend generative assistants with domain experts (e.g., AI‚Äëaugmented design tools, decision‚Äësupport dashboards) will dominate the enterprise software landscape, shifting the workforce toward higher‚Äëlevel creative and strategic tasks.\n",
       "\n",
       "### Societal Impact Projections\n",
       "- **Labor Market Re‚Äëskilling** ‚Äì Automation of routine analytics and decision processes will displace certain roles while creating demand for AI‚Äëfluency, data stewardship, and model‚Äëmaintenance expertise. Lifelong learning ecosystems will become essential.  \n",
       "- **Equity and Inclusion** ‚Äì If bias mitigation research translates into practice, ML‚Äëdriven services can reduce systemic disparities (e.g., fair credit scoring, unbiased medical diagnostics). Conversely, neglecting these safeguards could exacerbate existing inequities.  \n",
       "- **Privacy Paradigm Shift** ‚Äì Widespread adoption of privacy‚Äëpreserving ML will reshape expectations around personal data ownership, potentially leading to new legal frameworks and consumer empowerment tools.  \n",
       "- **Public Trust and Governance** ‚Äì Transparent model reporting, open‚Äësource audit tools, and participatory AI design processes will be critical to maintaining societal trust, especially in high‚Äëstakes domains like criminal justice or public health.  \n",
       "- **Environmental Stewardship** ‚Äì Energy‚Äëaware ML research and hardware innovations will mitigate the ecological footprint of AI, aligning the technology‚Äôs growth with global sustainability goals.\n",
       "\n",
       "Collectively, these trends suggest a near‚Äëterm transition from experimental ML prototypes to deeply integrated, responsibly governed intelligence that reshapes both industry operations and everyday life.\n",
       "\n",
       "---\n",
       "\n",
       "## Conclusion\n",
       "\n",
       "- **Key Takeaways**  \n",
       "  - Understanding the fundamentals of supervised, unsupervised, and reinforcement learning provides a solid foundation for tackling real‚Äëworld problems.  \n",
       "  - Mastery of data preprocessing, model selection, and evaluation metrics is essential to build robust, reliable systems.  \n",
       "  - Ethical considerations, interpretability, and scalability must be integrated into every stage of the machine‚Äëlearning pipeline.\n",
       "\n",
       "- **Why Mastery Matters**  \n",
       "  - Proficient ML practitioners can translate raw data into actionable insights, driving innovation across industries such as healthcare, finance, and transportation.  \n",
       "  - Deep knowledge reduces the risk of model bias, overfitting, and deployment failures, ensuring responsible and sustainable AI deployment.\n",
       "\n",
       "- **Recommendations for Further Study**  \n",
       "  1. **Advanced Topics** ‚Äì Dive into deep learning architectures (CNNs, RNNs, Transformers), probabilistic graphical models, and meta‚Äëlearning.  \n",
       "  2. **Practical Experience** ‚Äì Participate in Kaggle competitions, contribute to open‚Äësource projects, and build end‚Äëto‚Äëend pipelines on cloud platforms.  \n",
       "  3. **Theoretical Foundations** ‚Äì Study statistical learning theory, optimization algorithms, and information theory to strengthen intuition behind model behavior.  \n",
       "  4. **Ethics & Governance** ‚Äì Explore frameworks for fairness, accountability, and transparency; stay updated on emerging regulations.  \n",
       "  5. **Continuous Learning** ‚Äì Follow leading conferences (NeurIPS, ICML, CVPR), read recent arXiv papers, and engage with the ML community through webinars and meetups.\n",
       "\n",
       "By consolidating these concepts and pursuing the outlined pathways, learners can evolve from competent practitioners to thought leaders capable of shaping the future of machine learning.\n",
       "\n",
       "---\n",
       "\n",
       "## References\n",
       "\n",
       "### Books\n",
       "- **\"Artificial Intelligence: A Modern Approach\"** ‚Äì Stuart Russell & Peter Norvig (3rd‚ÄØed., 2020) ‚Äì Comprehensive textbook covering AI fundamentals, search algorithms, machine learning, and ethics.  \n",
       "- **\"Deep Learning\"** ‚Äì Ian Goodfellow, Yoshua Bengio & Aaron Courville (MIT Press, 2016) ‚Äì In‚Äëdepth treatment of neural networks, optimization, and modern architectures.  \n",
       "- **\"Pattern Recognition and Machine Learning\"** ‚Äì Christopher M. Bishop (Springer, 2006) ‚Äì Classic reference on statistical pattern recognition and Bayesian methods.  \n",
       "- **\"Data Science for Business\"** ‚Äì Foster Provost & Tom Fawcett (O'Reilly, 2013) ‚Äì Bridges the gap between data‚Äëdriven techniques and business strategy.  \n",
       "- **\"Hands‚ÄëOn Machine Learning with Scikit‚ÄëLearn, Keras, and TensorFlow\"** ‚Äì Aur√©lien G√©ron (O'Reilly, 2022) ‚Äì Practical guide with code examples for modern ML pipelines.\n",
       "\n",
       "### Research Papers\n",
       "- **LeCun, Y., Bengio, Y., & Hinton, G. (2015).** *Deep Learning.* **Nature, 521**, 436‚Äì444. DOI:10.1038/nature14539  \n",
       "- **Vaswani, A., et al. (2017).** *Attention Is All You Need.* **Advances in Neural Information Processing Systems, 30**, 5998‚Äë6008. arXiv:1706.03762  \n",
       "- **Brown, T. B., et al. (2020).** *Language Models are Few‚ÄëShot Learners.* **arXiv preprint arXiv:2005.14165**.  \n",
       "- **Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012).** *ImageNet Classification with Deep Convolutional Neural Networks.* **Communications of the ACM, 60(6)**, 84‚Äë90. DOI:10.1145/3065386  \n",
       "- **Raffel, C., et al. (2020).** *Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer.* **JMLR, 21(140)**, 1‚Äë67. arXiv:1910.10683  \n",
       "\n",
       "### Online Courses & MOOCs\n",
       "- **[Machine Learning by Stanford University (Coursera)](https://www.coursera.org/learn/machine-learning)** ‚Äì Andrew Ng‚Äôs foundational course covering supervised/unsupervised learning, neural networks, and best practices.  \n",
       "- **[Deep Learning Specialization (Coursera)](https://www.coursera.org/specializations/deep-learning)** ‚Äì Five‚Äëcourse series (Neural Networks, CNNs, Sequence Models, etc.) by Andrew Ng and team.  \n",
       "- **[CS231n: Convolutional Neural Networks for Visual Recognition (Stanford)](http://cs231n.stanford.edu/)** ‚Äì Lecture videos, notes, and assignments focusing on computer vision.  \n",
       "- **[Fast.ai Practical Deep Learning for Coders (free)](https://www.fast.ai/)** ‚Äì Hands‚Äëon approach using the fastai library and PyTorch.  \n",
       "- **[MIT 6.S191: Introduction to Deep Learning (MIT OpenCourseWare)](https://ocw.mit.edu/courses/6-s191-introduction-to-deep-learning-fall-2023/)** ‚Äì Lecture videos, labs, and assignments with a research‚Äëoriented perspective.\n",
       "\n",
       "### Reputable Websites & Resources\n",
       "- **[arXiv.org](https://arxiv.org/)** ‚Äì Pre‚Äëprint repository for the latest papers in AI, ML, statistics, and related fields.  \n",
       "- **[Papers with Code](https://paperswithcode.com/)** ‚Äì Links research papers to open‚Äësource implementations and benchmark leaderboards.  \n",
       "- **[DeepLearning.AI Blog](https://www.deeplearning.ai/blog/)** ‚Äì Articles, interviews, and updates from leading AI researchers.  \n",
       "- **[OpenAI Research](https://openai.com/research/)** ‚Äì Publications and technical reports on large‚Äëscale language models and safety.  \n",
       "- **[Google AI Blog](https://ai.googleblog.com/)** ‚Äì Insights into cutting‚Äëedge Google research, datasets, and tools.  \n",
       "\n",
       "These resources provide a solid foundation for further study, from theoretical underpinnings to practical implementation and current research trends."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = orchestrator_worker.invoke({\"topic\": \"crate a report on Machine Learning concpets\"})\n",
    "from IPython.display import Markdown\n",
    "Markdown(state[\"final_report\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ce865e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054c8373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6c3ef1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5814122a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a259153f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aae7b56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6584f3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1a37f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6daaa2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b2b727",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30f92ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb5443",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e19c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6de1fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df97467a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d2c354",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
